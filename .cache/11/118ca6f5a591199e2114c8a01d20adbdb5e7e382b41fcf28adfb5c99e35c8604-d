// Code generated by cmd/cgo; DO NOT EDIT.

//line /Users/z/go/pkg/mod/github.com/!data!dog/zstd@v1.5.7/zstd_stream.go:1:1
package zstd

/*
#include "zstd.h"

typedef struct compressStream2_result_s {
	size_t return_code;
	size_t bytes_consumed;
	size_t bytes_written;
} compressStream2_result;

static void ZSTD_compressStream2_wrapper(compressStream2_result* result, ZSTD_CCtx* ctx,
		void* dst, size_t maxDstSize, const void* src, size_t srcSize) {
	ZSTD_outBuffer outBuffer = { dst, maxDstSize, 0 };
	ZSTD_inBuffer inBuffer = { src, srcSize, 0 };
	size_t retCode = ZSTD_compressStream2(ctx, &outBuffer, &inBuffer, ZSTD_e_continue);

	result->return_code = retCode;
	result->bytes_consumed = inBuffer.pos;
	result->bytes_written = outBuffer.pos;
}

static void ZSTD_compressStream2_flush(compressStream2_result* result, ZSTD_CCtx* ctx,
		void* dst, size_t maxDstSize, const void* src, size_t srcSize) {
	ZSTD_outBuffer outBuffer = { dst, maxDstSize, 0 };
	ZSTD_inBuffer inBuffer = { src, srcSize, 0 };
	size_t retCode = ZSTD_compressStream2(ctx, &outBuffer, &inBuffer, ZSTD_e_flush);

	result->return_code = retCode;
	result->bytes_consumed = inBuffer.pos;
	result->bytes_written = outBuffer.pos;
}

static void ZSTD_compressStream2_finish(compressStream2_result* result, ZSTD_CCtx* ctx,
		void* dst, size_t maxDstSize, const void* src, size_t srcSize) {
	ZSTD_outBuffer outBuffer = { dst, maxDstSize, 0 };
	ZSTD_inBuffer inBuffer = { src, srcSize, 0 };
	size_t retCode = ZSTD_compressStream2(ctx, &outBuffer, &inBuffer, ZSTD_e_end);

	result->return_code = retCode;
	result->bytes_consumed = inBuffer.pos;
	result->bytes_written = outBuffer.pos;
}

// decompressStream2_result is the same as compressStream2_result, but keep 2 separate struct for easier changes
typedef struct decompressStream2_result_s {
	size_t return_code;
	size_t bytes_consumed;
	size_t bytes_written;
} decompressStream2_result;

static void ZSTD_decompressStream_wrapper(decompressStream2_result* result, ZSTD_DCtx* ctx,
		void* dst, size_t maxDstSize, const void* src, size_t srcSize) {
	ZSTD_outBuffer outBuffer = { dst, maxDstSize, 0 };
	ZSTD_inBuffer inBuffer = { src, srcSize, 0 };
	size_t retCode = ZSTD_decompressStream(ctx, &outBuffer, &inBuffer);

	result->return_code = retCode;
	result->bytes_consumed = inBuffer.pos;
	result->bytes_written = outBuffer.pos;
}
*/
import _ "unsafe"
import (
	"errors"
	"fmt"
	"io"
	"runtime"
	"sync"
	"unsafe"
)

var errShortRead = errors.New("short read")
var errReaderClosed = errors.New("Reader is closed")
var ErrNoParallelSupport = errors.New("No parallel support")

// Writer is an io.WriteCloser that zstd-compresses its input.
type Writer struct {
	CompressionLevel int

	ctx              * /*line :81:20*/_Ctype_ZSTD_CCtx /*line :81:31*/
	dict             []byte
	dstBuffer        []byte
	firstError       error
	underlyingWriter io.Writer
	resultBuffer     * /*line :86:20*/_Ctype_compressStream2_result /*line :86:44*/
}

func resize(in []byte, newSize int) []byte {
	if in == nil {
		return make([]byte, newSize)
	}
	if newSize <= cap(in) {
		return in[:newSize]
	}
	toAdd := newSize - len(in)
	return append(in, make([]byte, toAdd)...)
}

// NewWriter creates a new Writer with default compression options.  Writes to
// the writer will be written in compressed form to w.
func NewWriter(w io.Writer) *Writer {
	return NewWriterLevelDict(w, DefaultCompression, nil)
}

// NewWriterLevel is like NewWriter but specifies the compression level instead
// of assuming default compression.
//
// The level can be DefaultCompression or any integer value between BestSpeed
// and BestCompression inclusive.
func NewWriterLevel(w io.Writer, level int) *Writer {
	return NewWriterLevelDict(w, level, nil)

}

// NewWriterLevelDict is like NewWriterLevel but specifies a dictionary to
// compress with.  If the dictionary is empty or nil it is ignored. The dictionary
// should not be modified until the writer is closed.
func NewWriterLevelDict(w io.Writer, level int, dict []byte) *Writer {
	var err error
	ctx := ( /*line :121:9*/_Cfunc_ZSTD_createCStream /*line :121:28*/)()

	// Load dictionnary if any
	if dict != nil {
		err = getError(int(func() _Ctype_size_t{ _cgo0 := /*line :125:49*/ctx; _cgoIndex1 := &/*line :126:20*/dict; _cgo1 := /*line :126:4*/unsafe.Pointer(&(*_cgoIndex1)[0]); var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :127:12*/(len(dict)); _cgoCheckPointer(_cgo0, nil); _cgoCheckPointer(_cgo1, *_cgoIndex1); return /*line :128:4*/_Cfunc_ZSTD_CCtx_loadDictionary(_cgo0, _cgo1, _cgo2); }()))
	}

	if err == nil {
		// Only set level if the ctx is not in error already
		err = getError(int(func() _Ctype_size_t{ _cgo0 := /*line :133:47*/ctx; var _cgo1 _Ctype_ZSTD_cParameter = /*line :133:52*/_Ciconst_ZSTD_c_compressionLevel /*line :133:77*/; var _cgo2 _Ctype_int = _Ctype_int /*line :133:84*/(level); _cgoCheckPointer(_cgo0, nil); return /*line :133:92*/_Cfunc_ZSTD_CCtx_setParameter(_cgo0, _cgo1, _cgo2); }()))
	}

	return &Writer{
		CompressionLevel: level,
		ctx:              ctx,
		dict:             dict,
		dstBuffer:        make([]byte, CompressBound(1024)),
		firstError:       err,
		underlyingWriter: w,
		resultBuffer:     new( /*line :143:25*/_Ctype_compressStream2_result /*line :143:49*/),
	}
}

// Write writes a compressed form of p to the underlying io.Writer.
func (w *Writer) Write(p []byte) (int, error) {
	if w.firstError != nil {
		return 0, w.firstError
	}
	if len(p) == 0 {
		return 0, nil
	}
	total := len(p)
	// Check if dstBuffer is enough
	w.dstBuffer = w.dstBuffer[0:cap(w.dstBuffer)]
	if len(w.dstBuffer) < CompressBound(len(p)) {
		w.dstBuffer = make([]byte, CompressBound(len(p)))
	}

	dstoff := 0
	consumed := 0
	for len(p) > 0 {
		func() { var _cgo0 *_Ctype_struct_compressStream2_result_s = /*line :166:4*/w.resultBuffer; _cgo1 := /*line :167:4*/w.ctx; _cgoIndex2 := &/*line :168:20*/w.dstBuffer; _cgo2 := /*line :168:4*/unsafe.Pointer(&(*_cgoIndex2)[dstoff]); var _cgo3 _Ctype_size_t = _Ctype_size_t /*line :169:12*/(len(w.dstBuffer[dstoff:])); _cgoIndex4 := &/*line :170:20*/p; _cgo4 := /*line :170:4*/unsafe.Pointer(&(*_cgoIndex4)[0]); var _cgo5 _Ctype_size_t = _Ctype_size_t /*line :171:12*/(len(p)); _cgoCheckPointer(_cgo1, nil); _cgoCheckPointer(_cgo2, *_cgoIndex2); _cgoCheckPointer(_cgo4, *_cgoIndex4); /*line :172:4*/_Cfunc_ZSTD_compressStream2_wrapper(_cgo0, _cgo1, _cgo2, _cgo3, _cgo4, _cgo5); }()
		ret := int(w.resultBuffer.return_code)
		if err := getError(ret); err != nil {
			// The stream is dead after this.
			w.firstError = err
			return 0, err
		}
		p = p[w.resultBuffer.bytes_consumed:]
		dstoff += int(w.resultBuffer.bytes_written)
		consumed += int(w.resultBuffer.bytes_consumed)
		if len(p) > 0 && dstoff == len(w.dstBuffer) {
			// We have bytes remaining to compress and our output buffer
			// filled up. This shouldn't happen since we calculated it
			// in advance using CompressBound, but we need to handle it
			// in case there was some miscalculation, or the internal
			// stream buffer contained enough data from previous writes
			// to overflow dstBuffer. (it's not clear from the docs
			// whether this is possible)
			//
			// Allocate space for whatever we haven't compressed yet.
			newbuf := make([]byte, len(w.dstBuffer)+CompressBound(total-consumed))
			copy(newbuf, w.dstBuffer)
			w.dstBuffer = newbuf

		}
	}

	// Write to underlying buffer
	_, err := w.underlyingWriter.Write(w.dstBuffer[:dstoff])

	// Same behaviour as zlib, we can't know how much data we wrote, only
	// if there was an error
	if err != nil {
		return 0, err
	}
	return total, err
}

// Flush writes any unwritten data to the underlying io.Writer.
func (w *Writer) Flush() error {
	if w.firstError != nil {
		return w.firstError
	}

	ret := 1 // So we loop at least once
	for ret > 0 {
		func() { var _cgo0 *_Ctype_struct_compressStream2_result_s = /*line :219:4*/w.resultBuffer; _cgo1 := /*line :220:4*/w.ctx; _cgoIndex2 := &/*line :221:20*/w.dstBuffer; _cgo2 := /*line :221:4*/unsafe.Pointer(&(*_cgoIndex2)[0]); var _cgo3 _Ctype_size_t = _Ctype_size_t /*line :222:12*/(len(w.dstBuffer)); _cgo4 := /*line :223:4*/unsafe.Pointer(uintptr(0)); var _cgo5 _Ctype_size_t = _Ctype_size_t /*line :224:12*/(0); _cgoCheckPointer(_cgo1, nil); _cgoCheckPointer(_cgo2, *_cgoIndex2); _cgoCheckPointer(_cgo4, nil); /*line :225:4*/_Cfunc_ZSTD_compressStream2_flush(_cgo0, _cgo1, _cgo2, _cgo3, _cgo4, _cgo5); }()
		ret = int(w.resultBuffer.return_code)
		if err := getError(ret); err != nil {
			return err
		}
		written := int(w.resultBuffer.bytes_written)
		_, err := w.underlyingWriter.Write(w.dstBuffer[:written])
		if err != nil {
			return err
		}

		if ret > 0 { // We have a hint if we need to resize the dstBuffer
			w.dstBuffer = w.dstBuffer[:cap(w.dstBuffer)]
			if len(w.dstBuffer) < ret {
				w.dstBuffer = make([]byte, ret)
			}
		}
	}

	return nil
}

// Close closes the Writer, flushing any unwritten data to the underlying
// io.Writer and freeing objects, but does not close the underlying io.Writer.
func (w *Writer) Close() error {
	if w.firstError != nil {
		return w.firstError
	}

	ret := 1 // So we loop at least once
	for ret > 0 {
		func() { var _cgo0 *_Ctype_struct_compressStream2_result_s = /*line :257:4*/w.resultBuffer; _cgo1 := /*line :258:4*/w.ctx; _cgoIndex2 := &/*line :259:20*/w.dstBuffer; _cgo2 := /*line :259:4*/unsafe.Pointer(&(*_cgoIndex2)[0]); var _cgo3 _Ctype_size_t = _Ctype_size_t /*line :260:12*/(len(w.dstBuffer)); _cgo4 := /*line :261:4*/unsafe.Pointer(uintptr(0)); var _cgo5 _Ctype_size_t = _Ctype_size_t /*line :262:12*/(0); _cgoCheckPointer(_cgo1, nil); _cgoCheckPointer(_cgo2, *_cgoIndex2); _cgoCheckPointer(_cgo4, nil); /*line :263:4*/_Cfunc_ZSTD_compressStream2_finish(_cgo0, _cgo1, _cgo2, _cgo3, _cgo4, _cgo5); }()
		ret = int(w.resultBuffer.return_code)
		if err := getError(ret); err != nil {
			return err
		}
		written := int(w.resultBuffer.bytes_written)
		_, err := w.underlyingWriter.Write(w.dstBuffer[:written])
		if err != nil {
			func() _Ctype_size_t{ _cgo0 := /*line :271:23*/w.ctx; _cgoCheckPointer(_cgo0, nil); return /*line :271:29*/_Cfunc_ZSTD_freeCStream(_cgo0); }()
			return err
		}

		if ret > 0 { // We have a hint if we need to resize the dstBuffer
			w.dstBuffer = w.dstBuffer[:cap(w.dstBuffer)]
			if len(w.dstBuffer) < ret {
				w.dstBuffer = make([]byte, ret)
			}
		}
	}

	return getError(int(func() _Ctype_size_t{ _cgo0 := /*line :283:41*/w.ctx; _cgoCheckPointer(_cgo0, nil); return /*line :283:47*/_Cfunc_ZSTD_freeCStream(_cgo0); }()))
}

// Set the number of workers to run the compression in parallel using multiple threads
// If > 1, the Write() call will become asynchronous. This means data will be buffered until processed.
// If you call Write() too fast, you might incur a memory buffer up to as large as your input.
// Consider calling Flush() periodically if you need to compress a very large file that would not fit all in memory.
// By default only one worker is used.
func (w *Writer) SetNbWorkers(n int) error {
	if w.firstError != nil {
		return w.firstError
	}
	if err := getError(int(func() _Ctype_size_t{ _cgo0 := /*line :295:50*/w.ctx; var _cgo1 _Ctype_ZSTD_cParameter = /*line :295:57*/_Ciconst_ZSTD_c_nbWorkers /*line :295:75*/; var _cgo2 _Ctype_int = _Ctype_int /*line :295:82*/(n); _cgoCheckPointer(_cgo0, nil); return /*line :295:86*/_Cfunc_ZSTD_CCtx_setParameter(_cgo0, _cgo1, _cgo2); }())); err != nil {
		w.firstError = err
		// First error case, a shared libary is used, and the library was compiled without parallel support
		if err.Error() == "Unsupported parameter" {
			return ErrNoParallelSupport
		} else {
			// This could happen if a very large number is passed in, and possibly zstd refuse to create as many threads, or the OS fails to do so
			return err
		}
	}
	return nil
}

// cSize is the recommended size of reader.compressionBuffer. This func and
// invocation allow for a one-time check for validity.
var cSize = func() int {
	v := int(( /*line :311:11*/_Cfunc_ZSTD_DStreamInSize /*line :311:30*/)())
	if v <= 0 {
		panic(fmt.Errorf("ZSTD_DStreamInSize() returned invalid size: %v", v))
	}
	return v
}()

// dSize is the recommended size of reader.decompressionBuffer. This func and
// invocation allow for a one-time check for validity.
var dSize = func() int {
	v := int(( /*line :321:11*/_Cfunc_ZSTD_DStreamOutSize /*line :321:31*/)())
	if v <= 0 {
		panic(fmt.Errorf("ZSTD_DStreamOutSize() returned invalid size: %v", v))
	}
	return v
}()

// cPool is a pool of buffers for use in reader.compressionBuffer. Buffers are
// taken from the pool in NewReaderDict, returned in reader.Close(). Returns a
// pointer to a slice to avoid the extra allocation of returning the slice as a
// value.
var cPool = sync.Pool{
	New: func() interface{} {
		buff := make([]byte, cSize)
		return &buff
	},
}

// dPool is a pool of buffers for use in reader.decompressionBuffer. Buffers are
// taken from the pool in NewReaderDict, returned in reader.Close(). Returns a
// pointer to a slice to avoid the extra allocation of returning the slice as a
// value.
var dPool = sync.Pool{
	New: func() interface{} {
		buff := make([]byte, dSize)
		return &buff
	},
}

// reader is an io.ReadCloser that decompresses when read from.
type reader struct {
	ctx                 * /*line :352:23*/_Ctype_ZSTD_DCtx /*line :352:34*/
	compressionBuffer   []byte
	compressionLeft     int
	decompressionBuffer []byte
	decompOff           int
	decompSize          int
	dict                []byte
	firstError          error
	recommendedSrcSize  int
	resultBuffer        * /*line :361:23*/_Ctype_decompressStream2_result /*line :361:49*/
	underlyingReader    io.Reader
}

// NewReader creates a new io.ReadCloser.  Reads from the returned ReadCloser
// read and decompress data from r.  It is the caller's responsibility to call
// Close on the ReadCloser when done.  If this is not done, underlying objects
// in the zstd library will not be freed.
func NewReader(r io.Reader) io.ReadCloser {
	return NewReaderDict(r, nil)
}

// NewReaderDict is like NewReader but uses a preset dictionary.  NewReaderDict
// ignores the dictionary if it is nil.
func NewReaderDict(r io.Reader, dict []byte) io.ReadCloser {
	var err error
	ctx := ( /*line :377:9*/_Cfunc_ZSTD_createDStream /*line :377:28*/)()
	if len(dict) == 0 {
		err = getError(int(func() _Ctype_size_t{ _cgo0 := /*line :379:41*/ctx; _cgoCheckPointer(_cgo0, nil); return /*line :379:45*/_Cfunc_ZSTD_initDStream(_cgo0); }()))
	} else {
		err = getError(int(func() _Ctype_size_t{ _cgo0 := /*line :381:40*/ctx; var _cgo1 _Ctype_ZSTD_ResetDirective = /*line :381:45*/_Ciconst_ZSTD_reset_session_only /*line :381:70*/; _cgoCheckPointer(_cgo0, nil); return /*line :381:71*/_Cfunc_ZSTD_DCtx_reset(_cgo0, _cgo1); }()))
		if err == nil {
			// Only load dictionary if we succesfully inited the context
			err = getError(int(func() _Ctype_size_t{ _cgo0 := /*line :385:5*/ctx; _cgoIndex1 := &/*line :386:21*/dict; _cgo1 := /*line :386:5*/unsafe.Pointer(&(*_cgoIndex1)[0]); var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :387:13*/(len(dict)); _cgoCheckPointer(_cgo0, nil); _cgoCheckPointer(_cgo1, *_cgoIndex1); return /*line :387:25*/_Cfunc_ZSTD_DCtx_loadDictionary(_cgo0, _cgo1, _cgo2); }()))
		}
	}
	compressionBufferP := cPool.Get().(*[]byte)
	decompressionBufferP := dPool.Get().(*[]byte)
	return &reader{
		ctx:                 ctx,
		dict:                dict,
		compressionBuffer:   *compressionBufferP,
		decompressionBuffer: *decompressionBufferP,
		firstError:          err,
		recommendedSrcSize:  cSize,
		resultBuffer:        new( /*line :399:28*/_Ctype_decompressStream2_result /*line :399:54*/),
		underlyingReader:    r,
	}
}

// Close frees the allocated C objects
func (r *reader) Close() error {
	if r.firstError != nil {
		return r.firstError
	}

	cb := r.compressionBuffer
	db := r.decompressionBuffer
	// Ensure that we won't resuse buffer
	r.firstError = errReaderClosed
	r.compressionBuffer = nil
	r.decompressionBuffer = nil

	cPool.Put(&cb)
	dPool.Put(&db)
	return getError(int(func() _Ctype_size_t{ _cgo0 := /*line :419:41*/r.ctx; _cgoCheckPointer(_cgo0, nil); return /*line :419:47*/_Cfunc_ZSTD_freeDStream(_cgo0); }()))
}

func (r *reader) Read(p []byte) (int, error) {
	if r.firstError != nil {
		return 0, r.firstError
	}

	if len(p) == 0 {
		return 0, nil
	}

	// If we already have some uncompressed bytes, return without blocking
	if r.decompSize > r.decompOff {
		if r.decompSize-r.decompOff > len(p) {
			copy(p, r.decompressionBuffer[r.decompOff:])
			r.decompOff += len(p)
			return len(p), nil
		}
		// From https://golang.org/pkg/io/#Reader
		// > Read conventionally returns what is available instead of waiting for more.
		copy(p, r.decompressionBuffer[r.decompOff:r.decompSize])
		got := r.decompSize - r.decompOff
		r.decompOff = r.decompSize
		return got, nil
	}

	// Repeatedly read from the underlying reader until we get
	// at least one zstd block, so that we don't block if the
	// other end has flushed a block.
	for {
		// - If the last decompression didn't entirely fill the decompression buffer,
		//   zstd flushed all it could, and needs new data. In that case, do 1 Read.
		// - If the last decompression did entirely fill the decompression buffer,
		//   it might have needed more room to decompress the input. In that case,
		//   don't do any unnecessary Read that might block.
		needsData := r.decompSize < len(r.decompressionBuffer)

		var src []byte
		if !needsData {
			src = r.compressionBuffer[:r.compressionLeft]
		} else {
			src = r.compressionBuffer
			var n int
			var err error
			// Read until data arrives or an error occurs.
			for n == 0 && err == nil {
				n, err = r.underlyingReader.Read(src[r.compressionLeft:])
			}
			if err != nil && err != io.EOF { // Handle underlying reader errors first
				return 0, fmt.Errorf("failed to read from underlying reader: %w", err)
			}
			if n == 0 {
				// Ideally, we'd return with ErrUnexpectedEOF in all cases where the stream was unexpectedly EOF'd
				// during a block or frame, i.e. when there are incomplete, pending compression data.
				// However, it's hard to detect those cases with zstd. Namely, there is no way to know the size of
				// the current buffered compression data in the zstd stream internal buffers.
				// Best effort: throw ErrUnexpectedEOF if we still have some pending buffered compression data that
				// zstd doesn't want to accept.
				// If we don't have any buffered compression data but zstd still has some in its internal buffers,
				// we will return with EOF instead.
				if r.compressionLeft > 0 {
					return 0, io.ErrUnexpectedEOF
				}
				return 0, io.EOF
			}
			src = src[:r.compressionLeft+n]
		}

		// C code
		var srcPtr *byte // Do not point anywhere, if src is empty
		if len(src) > 0 {
			srcPtr = &src[0]
		}

		func() { var _cgo0 *_Ctype_struct_decompressStream2_result_s = /*line :495:4*/r.resultBuffer; _cgo1 := /*line :496:4*/r.ctx; _cgoIndex2 := &/*line :497:20*/r.decompressionBuffer; _cgo2 := /*line :497:4*/unsafe.Pointer(&(*_cgoIndex2)[0]); var _cgo3 _Ctype_size_t = _Ctype_size_t /*line :498:12*/(len(r.decompressionBuffer)); _cgo4 := /*line :499:4*/unsafe.Pointer(srcPtr); var _cgo5 _Ctype_size_t = _Ctype_size_t /*line :500:12*/(len(src)); _cgoCheckPointer(_cgo1, nil); _cgoCheckPointer(_cgo2, *_cgoIndex2); _cgoCheckPointer(_cgo4, nil); /*line :501:4*/_Cfunc_ZSTD_decompressStream_wrapper(_cgo0, _cgo1, _cgo2, _cgo3, _cgo4, _cgo5); }()
		retCode := int(r.resultBuffer.return_code)

		// Keep src here even though we reuse later, the code might be deleted at some point
		runtime.KeepAlive(src)
		if err := getError(retCode); err != nil {
			return 0, fmt.Errorf("failed to decompress: %w", err)
		}

		// Put everything in buffer
		bytesConsumed := int(r.resultBuffer.bytes_consumed)
		if bytesConsumed < len(src) {
			left := src[bytesConsumed:]
			copy(r.compressionBuffer, left)
		}
		r.compressionLeft = len(src) - bytesConsumed
		r.decompSize = int(r.resultBuffer.bytes_written)
		r.decompOff = copy(p, r.decompressionBuffer[:r.decompSize])

		// Resize buffers
		nsize := retCode // Hint for next src buffer size
		if nsize <= 0 {
			// Reset to recommended size
			nsize = r.recommendedSrcSize
		}
		if nsize < r.compressionLeft {
			nsize = r.compressionLeft
		}
		r.compressionBuffer = resize(r.compressionBuffer, nsize)

		if r.decompOff > 0 {
			return r.decompOff, nil
		}
	}
}
