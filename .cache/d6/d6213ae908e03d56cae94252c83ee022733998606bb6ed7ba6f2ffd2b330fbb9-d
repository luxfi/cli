// Code generated by cmd/cgo; DO NOT EDIT.

//line /Users/z/go/pkg/mod/github.com/supranational/blst@v0.3.15/bindings/go/blst.go:1:1
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
// DO NOT MODIFY THIS FILE!!
// The file is generated from *.tgo by generate.py
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
/*
 * Copyright Supranational LLC
 * Licensed under the Apache License, Version 2.0, see LICENSE for details.
 * SPDX-License-Identifier: Apache-2.0
 */

package blst

// #cgo CFLAGS: -I${SRCDIR}/.. -I${SRCDIR}/../../build -I${SRCDIR}/../../src -D__BLST_CGO__ -fno-builtin-memcpy -fno-builtin-memset
// #cgo amd64 CFLAGS: -D__ADX__ -mno-avx
// // no-asm 64-bit platforms from https://go.dev/doc/install/source
// #cgo loong64 mips64 mips64le ppc64 ppc64le riscv64 s390x CFLAGS: -D__BLST_NO_ASM__
//
// #include "blst.h"
//
// #if defined(__x86_64__) && (defined(__unix__) || defined(__APPLE__))
// # include <signal.h>
// # include <unistd.h>
// static void handler(int signum)
// {   ssize_t n = write(2, "Caught SIGILL in blst_cgo_init, "
//                          "consult <blst>/bindings/go/README.md.\n", 70);
//     _exit(128+SIGILL);
//     (void)n;
// }
// __attribute__((constructor)) static void blst_cgo_init()
// {   blst_fp temp = { 0 };
//     struct sigaction act = { handler }, oact;
//     sigaction(SIGILL, &act, &oact);
//     blst_fp_sqr(&temp, &temp);
//     sigaction(SIGILL, &oact, NULL);
// }
// #endif
//
// static void go_pairing_init(blst_pairing *new_ctx, bool hash_or_encode,
//                             const byte *DST, size_t DST_len)
// {   if (DST != NULL) {
//         byte *dst = (byte*)new_ctx + blst_pairing_sizeof();
//         for(size_t i = 0; i < DST_len; i++) dst[i] = DST[i];
//         DST = dst;
//     }
//     blst_pairing_init(new_ctx, hash_or_encode, DST, DST_len);
// }
// static void go_pairing_as_fp12(blst_fp12 *pt, blst_pairing *ctx)
// {   *pt = *blst_pairing_as_fp12(ctx);   }
//
// static void go_p1slice_to_affine(blst_p1_affine dst[],
//                                  const blst_p1 points[], size_t npoints)
// {   const blst_p1 *ppoints[2] = { points, NULL };
//     blst_p1s_to_affine(dst, ppoints, npoints);
// }
// static void go_p1slice_add(blst_p1 *dst, const blst_p1_affine points[],
//                                          size_t npoints)
// {   const blst_p1_affine *ppoints[2] = { points, NULL };
//     blst_p1s_add(dst, ppoints, npoints);
// }
// static void go_p2slice_to_affine(blst_p2_affine dst[],
//                                  const blst_p2 points[], size_t npoints)
// {   const blst_p2 *ppoints[2] = { points, NULL };
//     blst_p2s_to_affine(dst, ppoints, npoints);
// }
// static void go_p2slice_add(blst_p2 *dst, const blst_p2_affine points[],
//                                          size_t npoints)
// {   const blst_p2_affine *ppoints[2] = { points, NULL };
//     blst_p2s_add(dst, ppoints, npoints);
// }
//
// static void go_p1_mult_n_acc(blst_p1 *acc, const blst_fp *x, bool affine,
//                                            const byte *scalar, size_t nbits)
// {   blst_p1 m[1];
//     const void *p = x;
//     if (p == NULL)
//         p = blst_p1_generator();
//     else if (affine)
//         blst_p1_from_affine(m, p), p = m;
//     blst_p1_mult(m, p, scalar, nbits);
//     blst_p1_add_or_double(acc, acc, m);
// }
// static void go_p2_mult_n_acc(blst_p2 *acc, const blst_fp2 *x, bool affine,
//                                            const byte *scalar, size_t nbits)
// {   blst_p2 m[1];
//     const void *p = x;
//     if (p == NULL)
//         p = blst_p2_generator();
//     else if (affine)
//         blst_p2_from_affine(m, p), p = m;
//     blst_p2_mult(m, p, scalar, nbits);
//     blst_p2_add_or_double(acc, acc, m);
// }
//
// static void go_p1_sub_assign(blst_p1 *a, const blst_fp *x, bool affine)
// {   blst_p1 minus_b;
//     if (affine)
//         blst_p1_from_affine(&minus_b, (const blst_p1_affine*)x);
//     else
//         minus_b = *(const blst_p1*)x;
//     blst_p1_cneg(&minus_b, 1);
//     blst_p1_add_or_double(a, a, &minus_b);
// }
//
// static void go_p2_sub_assign(blst_p2 *a, const blst_fp2 *x, bool affine)
// {   blst_p2 minus_b;
//     if (affine)
//         blst_p2_from_affine(&minus_b, (const blst_p2_affine*)x);
//     else
//         minus_b = *(const blst_p2*)x;
//     blst_p2_cneg(&minus_b, 1);
//     blst_p2_add_or_double(a, a, &minus_b);
// }
//
// static bool go_scalar_from_bendian(blst_scalar *ret, const byte *in)
// {   blst_scalar_from_bendian(ret, in);
//     return blst_sk_check(ret);
// }
// static bool go_hash_to_scalar(blst_scalar *ret,
//                               const byte *msg, size_t msg_len,
//                               const byte *DST, size_t DST_len)
// {   byte elem[48];
//     blst_expand_message_xmd(elem, sizeof(elem), msg, msg_len, DST, DST_len);
//     return blst_scalar_from_be_bytes(ret, elem, sizeof(elem));
// }
// static void go_miller_loop_n(blst_fp12 *dst, const blst_p2_affine Q[],
//                                              const blst_p1_affine P[],
//                                              size_t npoints, bool acc)
// {   const blst_p2_affine *Qs[2] = { Q, NULL };
//     const blst_p1_affine *Ps[2] = { P, NULL };
//     if (acc) {
//         blst_fp12 tmp;
//         blst_miller_loop_n(&tmp, Qs, Ps, npoints);
//         blst_fp12_mul(dst, dst, &tmp);
//     } else {
//         blst_miller_loop_n(dst, Qs, Ps, npoints);
//     }
// }
// static void go_fp12slice_mul(blst_fp12 *dst, const blst_fp12 in[], size_t n)
// {   size_t i;
//     blst_fp12_mul(dst, &in[0], &in[1]);
//     for (i = 2; i < n; i++)
//         blst_fp12_mul(dst, dst, &in[i]);
// }
// static bool go_p1_affine_validate(const blst_p1_affine *p, bool infcheck)
// {   if (infcheck && blst_p1_affine_is_inf(p))
//         return 0;
//     return blst_p1_affine_in_g1(p);
// }
// static bool go_p2_affine_validate(const blst_p2_affine *p, bool infcheck)
// {   if (infcheck && blst_p2_affine_is_inf(p))
//         return 0;
//     return blst_p2_affine_in_g2(p);
// }
import _ "unsafe"

import (
	"fmt"
	"math/bits"
	"runtime"
	"sync"
	"sync/atomic"
	"unsafe"
)

const BLST_SCALAR_BYTES = 256 / 8
const BLST_FP_BYTES = 384 / 8
const BLST_P1_COMPRESS_BYTES = BLST_FP_BYTES
const BLST_P1_SERIALIZE_BYTES = BLST_FP_BYTES * 2
const BLST_P2_COMPRESS_BYTES = BLST_FP_BYTES * 2
const BLST_P2_SERIALIZE_BYTES = BLST_FP_BYTES * 4

type Scalar struct{ cgo  /*line :172:25*/_Ctype_blst_scalar /*line :172:38*/ }
type Fp struct{ cgo  /*line :173:21*/_Ctype_blst_fp /*line :173:30*/ }
type Fp2 struct{ cgo  /*line :174:22*/_Ctype_blst_fp2 /*line :174:32*/ }
type Fp6 =  /*line :175:12*/_Ctype_blst_fp6 /*line :175:22*/
type Fp12 struct{ cgo  /*line :176:23*/_Ctype_blst_fp12 /*line :176:34*/ }
type P1 struct{ cgo  /*line :177:21*/_Ctype_blst_p1 /*line :177:30*/ }
type P2 struct{ cgo  /*line :178:21*/_Ctype_blst_p2 /*line :178:30*/ }
type P1Affine struct{ cgo  /*line :179:27*/_Ctype_blst_p1_affine /*line :179:43*/ }
type P2Affine struct{ cgo  /*line :180:27*/_Ctype_blst_p2_affine /*line :180:43*/ }
type Message = []byte
type Pairing = [] /*line :182:18*/_Ctype_blst_pairing /*line :182:32*/
type SecretKey = Scalar
type P1s []P1
type P2s []P2
type P1Affines []P1Affine
type P2Affines []P2Affine

//
// Configuration
//

var maxProcs = initMaxProcs()

func initMaxProcs() int {
	maxProcs := runtime.GOMAXPROCS(0)
	var version float32
	_, err := fmt.Sscanf(runtime.Version(), "go%f", &version)
	if err != nil || version < 1.14 {
		// be cooperative and leave one processor for the application
		maxProcs -= 1
	}
	if maxProcs <= 0 {
		maxProcs = 1
	}
	return maxProcs
}

func SetMaxProcs(procs int) {
	if procs <= 0 {
		procs = 1
	}
	maxProcs = procs
}

func numThreads(maxThreads int) int {
	numThreads := maxProcs

	// take into consideration the possility that application reduced
	// GOMAXPROCS after |maxProcs| was initialized
	numProcs := runtime.GOMAXPROCS(0)
	if maxProcs > numProcs {
		numThreads = numProcs
	}

	if maxThreads > 0 && numThreads > maxThreads {
		return maxThreads
	}
	return numThreads
}

var cgo_pairingSizeOf = ( /*line :232:25*/_Cfunc_blst_pairing_sizeof /*line :232:45*/)()
var cgo_p1Generator = P1{*( /*line :233:27*/_Cfunc_blst_p1_generator /*line :233:45*/)()}
var cgo_p2Generator = P2{*( /*line :234:27*/_Cfunc_blst_p2_generator /*line :234:45*/)()}
var cgo_fp12One = Fp12{*( /*line :235:25*/_Cfunc_blst_fp12_one /*line :235:39*/)()}

// Secret key
func (sk *SecretKey) Zeroize() {
	var zero SecretKey
	*sk = zero
}

func KeyGen(ikm []byte, optional ...[]byte) *SecretKey {
	var sk SecretKey
	var info []byte
	if len(optional) > 0 {
		info = optional[0]
	}
	if len(ikm) < 32 {
		return nil
	}
	( /*line :252:2*/_Cfunc_blst_keygen /*line :252:14*/)(&sk.cgo, (* /*line :252:27*/_Ctype_byte /*line :252:33*/)(&ikm[0]),  /*line :252:45*/_Ctype_size_t /*line :252:53*/(len(ikm)),
		ptrOrNil(info),  /*line :253:19*/_Ctype_size_t /*line :253:27*/(len(info)))
	// Postponing secret key zeroing till garbage collection can be too
	// late to be effective, but every little bit helps...
	runtime.SetFinalizer(&sk, func(sk *SecretKey) { sk.Zeroize() })
	return &sk
}

func KeyGenV3(ikm []byte, optional ...[]byte) *SecretKey {
	if len(ikm) < 32 {
		return nil
	}
	var sk SecretKey
	var info []byte
	if len(optional) > 0 {
		info = optional[0]
	}
	( /*line :269:2*/_Cfunc_blst_keygen_v3 /*line :269:17*/)(&sk.cgo, (* /*line :269:30*/_Ctype_byte /*line :269:36*/)(&ikm[0]),  /*line :269:48*/_Ctype_size_t /*line :269:56*/(len(ikm)),
		ptrOrNil(info),  /*line :270:19*/_Ctype_size_t /*line :270:27*/(len(info)))
	// Postponing secret key zeroing till garbage collection can be too
	// late to be effective, but every little bit helps...
	runtime.SetFinalizer(&sk, func(sk *SecretKey) { sk.Zeroize() })
	return &sk
}

func KeyGenV45(ikm []byte, salt []byte, optional ...[]byte) *SecretKey {
	if len(ikm) < 32 {
		return nil
	}
	var sk SecretKey
	var info []byte
	if len(optional) > 0 {
		info = optional[0]
	}
	( /*line :286:2*/_Cfunc_blst_keygen_v4_5 /*line :286:19*/)(&sk.cgo, (* /*line :286:32*/_Ctype_byte /*line :286:38*/)(&ikm[0]),  /*line :286:50*/_Ctype_size_t /*line :286:58*/(len(ikm)),
		(* /*line :287:5*/_Ctype_byte /*line :287:11*/)(&salt[0]),  /*line :287:24*/_Ctype_size_t /*line :287:32*/(len(salt)),
		ptrOrNil(info),  /*line :288:19*/_Ctype_size_t /*line :288:27*/(len(info)))
	// Postponing secret key zeroing till garbage collection can be too
	// late to be effective, but every little bit helps...
	runtime.SetFinalizer(&sk, func(sk *SecretKey) { sk.Zeroize() })
	return &sk
}

func KeyGenV5(ikm []byte, salt []byte, optional ...[]byte) *SecretKey {
	if len(ikm) < 32 {
		return nil
	}
	var sk SecretKey
	var info []byte
	if len(optional) > 0 {
		info = optional[0]
	}
	( /*line :304:2*/_Cfunc_blst_keygen_v5 /*line :304:17*/)(&sk.cgo, (* /*line :304:30*/_Ctype_byte /*line :304:36*/)(&ikm[0]),  /*line :304:48*/_Ctype_size_t /*line :304:56*/(len(ikm)),
		(* /*line :305:5*/_Ctype_byte /*line :305:11*/)(&salt[0]),  /*line :305:24*/_Ctype_size_t /*line :305:32*/(len(salt)),
		ptrOrNil(info),  /*line :306:19*/_Ctype_size_t /*line :306:27*/(len(info)))
	// Postponing secret key zeroing till garbage collection can be too
	// late to be effective, but every little bit helps...
	runtime.SetFinalizer(&sk, func(sk *SecretKey) { sk.Zeroize() })
	return &sk
}

func DeriveMasterEip2333(ikm []byte) *SecretKey {
	if len(ikm) < 32 {
		return nil
	}
	var sk SecretKey
	( /*line :318:2*/_Cfunc_blst_derive_master_eip2333 /*line :318:29*/)(&sk.cgo, (* /*line :318:42*/_Ctype_byte /*line :318:48*/)(&ikm[0]),  /*line :318:60*/_Ctype_size_t /*line :318:68*/(len(ikm)))
	// Postponing secret key zeroing till garbage collection can be too
	// late to be effective, but every little bit helps...
	runtime.SetFinalizer(&sk, func(sk *SecretKey) { sk.Zeroize() })
	return &sk
}

func (master *SecretKey) DeriveChildEip2333(child_index uint32) *SecretKey {
	var sk SecretKey
	( /*line :327:2*/_Cfunc_blst_derive_child_eip2333 /*line :327:28*/)(&sk.cgo, &master.cgo,  /*line :327:52*/_Ctype_uint /*line :327:58*/(child_index))
	// Postponing secret key zeroing till garbage collection can be too
	// late to be effective, but every little bit helps...
	runtime.SetFinalizer(&sk, func(sk *SecretKey) { sk.Zeroize() })
	return &sk
}

// Pairing
func pairingSizeOf(DST_len  /*line :335:28*/_Ctype_size_t /*line :335:36*/) int {
	return int((cgo_pairingSizeOf + DST_len + 7) / 8)
}

func PairingCtx(hash_or_encode bool, DST []byte) Pairing {
	DST_len :=  /*line :340:13*/_Ctype_size_t /*line :340:21*/(len(DST))
	ctx := make([] /*line :341:16*/_Ctype_blst_pairing /*line :341:30*/, pairingSizeOf(DST_len))
	( /*line :342:2*/_Cfunc_go_pairing_init /*line :342:18*/)(&ctx[0],  /*line :342:29*/_Ctype_bool /*line :342:35*/(hash_or_encode), ptrOrNil(DST), DST_len)
	return ctx
}

func PairingCommit(ctx Pairing) {
	( /*line :347:2*/_Cfunc_blst_pairing_commit /*line :347:22*/)(&ctx[0])
}

func PairingMerge(ctx Pairing, ctx1 Pairing) int {
	r := ( /*line :351:7*/_Cfunc_blst_pairing_merge /*line :351:26*/)(&ctx[0], &ctx1[0])
	return int(r)
}

func PairingFinalVerify(ctx Pairing, optional ...*Fp12) bool {
	var gtsig *Fp12
	if len(optional) > 0 {
		gtsig = optional[0]
	}
	return bool(( /*line :360:14*/_Cfunc_blst_pairing_finalverify /*line :360:39*/)(&ctx[0], gtsig.asPtr()))
}

func PairingRawAggregate(ctx Pairing, q *P2Affine, p *P1Affine) {
	( /*line :364:2*/_Cfunc_blst_pairing_raw_aggregate /*line :364:29*/)(&ctx[0], &q.cgo, &p.cgo)
}

func PairingAsFp12(ctx Pairing) *Fp12 {
	var pt Fp12
	( /*line :369:2*/_Cfunc_go_pairing_as_fp12 /*line :369:21*/)(&pt.cgo, &ctx[0])
	return &pt
}

func Fp12One() Fp12 {
	return cgo_fp12One
}

func Fp12FinalVerify(pt1 *Fp12, pt2 *Fp12) bool {
	return bool(( /*line :378:14*/_Cfunc_blst_fp12_finalverify /*line :378:36*/)(&pt1.cgo, &pt2.cgo))
}

func Fp12MillerLoop(q *P2Affine, p *P1Affine) *Fp12 {
	var pt Fp12
	( /*line :383:2*/_Cfunc_blst_miller_loop /*line :383:19*/)(&pt.cgo, &q.cgo, &p.cgo)
	return &pt
}

func Fp12MillerLoopN(qs []P2Affine, ps []P1Affine) *Fp12 {
	if len(qs) != len(ps) || len(qs) == 0 {
		panic("inputs' lengths mismatch")
	}

	nElems := uint32(len(qs))
	nThreads := uint32(maxProcs)

	if nThreads == 1 || nElems == 1 {
		var pt Fp12
		( /*line :397:3*/_Cfunc_go_miller_loop_n /*line :397:20*/)(&pt.cgo, &qs[0].cgo, &ps[0].cgo,  /*line :397:55*/_Ctype_size_t /*line :397:63*/(nElems), false)
		return &pt
	}

	stride := (nElems + nThreads - 1) / nThreads
	if stride > 16 {
		stride = 16
	}

	strides := (nElems + stride - 1) / stride
	if nThreads > strides {
		nThreads = strides
	}

	msgsCh := make(chan Fp12, nThreads)
	curElem := uint32(0)

	for tid := uint32(0); tid < nThreads; tid++ {
		go func() {
			acc := Fp12One()
			first := true
			for {
				work := atomic.AddUint32(&curElem, stride) - stride
				if work >= nElems {
					break
				}
				n := nElems - work
				if n > stride {
					n = stride
				}
				( /*line :427:5*/_Cfunc_go_miller_loop_n /*line :427:22*/)(&acc.cgo, &qs[work].cgo, &ps[work].cgo,  /*line :427:64*/_Ctype_size_t /*line :427:72*/(n),
					 /*line :428:6*/_Ctype_bool /*line :428:12*/(!first))
				first = false
			}
			msgsCh <- acc
		}()
	}

	var ret = make([]Fp12, nThreads)
	for i := range ret {
		ret[i] = <-msgsCh
	}

	var pt Fp12
	( /*line :441:2*/_Cfunc_go_fp12slice_mul /*line :441:19*/)(&pt.cgo, &ret[0].cgo,  /*line :441:43*/_Ctype_size_t /*line :441:51*/(nThreads))
	return &pt
}

func (pt *Fp12) MulAssign(p *Fp12) {
	( /*line :446:2*/_Cfunc_blst_fp12_mul /*line :446:16*/)(&pt.cgo, &pt.cgo, &p.cgo)
}

func (pt *Fp12) FinalExp() {
	( /*line :450:2*/_Cfunc_blst_final_exp /*line :450:17*/)(&pt.cgo, &pt.cgo)
}

func (pt *Fp12) InGroup() bool {
	return bool(( /*line :454:14*/_Cfunc_blst_fp12_in_group /*line :454:33*/)(&pt.cgo))
}

func (pt *Fp12) ToBendian() []byte {
	var out [BLST_FP_BYTES * 12]byte
	( /*line :459:2*/_Cfunc_blst_bendian_from_fp12 /*line :459:25*/)((* /*line :459:29*/_Ctype_byte /*line :459:35*/)(&out[0]), &pt.cgo)
	return out[:]
}

func (pt1 *Fp12) Equals(pt2 *Fp12) bool {
	return *pt1 == *pt2
}

func (pt *Fp12) asPtr() * /*line :467:26*/_Ctype_blst_fp12 /*line :467:37*/ {
	if pt != nil {
		return &pt.cgo
	}

	return nil
}

func ptrOrNil(bytes []byte) * /*line :475:30*/_Ctype_byte /*line :475:36*/ {
	var ptr * /*line :476:11*/_Ctype_byte /*line :476:17*/
	if len(bytes) > 0 {
		ptr = (* /*line :478:11*/_Ctype_byte /*line :478:17*/)(&bytes[0])
	}
	return ptr
}

//
// MIN-PK
//

//
// PublicKey
//

func (pk *P1Affine) From(s *Scalar) *P1Affine {
	( /*line :492:2*/_Cfunc_blst_sk_to_pk2_in_g1 /*line :492:23*/)(nil, &pk.cgo, &s.cgo)
	return pk
}

func (pk *P1Affine) KeyValidate() bool {
	return bool(( /*line :497:14*/_Cfunc_go_p1_affine_validate /*line :497:36*/)(&pk.cgo, true))
}

// sigInfcheck, check for infinity, is a way to avoid going
// into resource-consuming verification. Passing 'false' is
// always cryptographically safe, but application might want
// to guard against obviously bogus individual[!] signatures.
func (sig *P2Affine) SigValidate(sigInfcheck bool) bool {
	return bool(( /*line :505:14*/_Cfunc_go_p2_affine_validate /*line :505:36*/)(&sig.cgo,  /*line :505:48*/_Ctype_bool /*line :505:54*/(sigInfcheck)))
}

//
// Sign
//

func (sig *P2Affine) Sign(sk *SecretKey, msg []byte, dst []byte,
	optional ...interface{}) *P2Affine {
	augSingle, aug, useHash, ok := parseOpts(optional...)
	if !ok || len(aug) != 0 {
		return nil
	}

	var q *P2
	if useHash {
		q = HashToG2(msg, dst, augSingle)
	} else {
		q = EncodeToG2(msg, dst, augSingle)
	}
	( /*line :525:2*/_Cfunc_blst_sign_pk2_in_g1 /*line :525:22*/)(nil, &sig.cgo, &q.cgo, &sk.cgo)
	return sig
}

//
// Signature
//

// Functions to return a signature and public key+augmentation tuple.
// This enables point decompression (if needed) to happen in parallel.
type sigGetterP2 func() *P2Affine
type pkGetterP1 func(i uint32, temp *P1Affine) (*P1Affine, []byte)

// Single verify with decompressed pk
func (sig *P2Affine) Verify(sigGroupcheck bool, pk *P1Affine, pkValidate bool,
	msg Message, dst []byte,
	optional ...interface{}) bool { // useHash bool, aug []byte

	aug, _, useHash, ok := parseOpts(optional...)
	if !ok {
		return false
	}
	return sig.AggregateVerify(sigGroupcheck, []*P1Affine{pk}, pkValidate,
		[]Message{msg}, dst, useHash, [][]byte{aug})
}

// Single verify with compressed pk
// Uses a dummy signature to get the correct type
func (dummy *P2Affine) VerifyCompressed(sig []byte, sigGroupcheck bool,
	pk []byte, pkValidate bool, msg Message, dst []byte,
	optional ...bool) bool { // useHash bool, usePksAsAugs bool

	return dummy.AggregateVerifyCompressed(sig, sigGroupcheck,
		[][]byte{pk}, pkValidate,
		[]Message{msg}, dst, optional...)
}

// Aggregate verify with uncompressed signature and public keys
// Note that checking message uniqueness, if required, is left to the user.
// Not all signature schemes require it and this keeps the binding minimal
// and fast. Refer to the Uniq function for one method method of performing
// this check.
func (sig *P2Affine) AggregateVerify(sigGroupcheck bool,
	pks []*P1Affine, pksVerify bool, msgs []Message, dst []byte,
	optional ...interface{}) bool { // useHash bool, augs [][]byte

	// sanity checks and argument parsing
	n := len(pks)
	if n == 0 || len(msgs) != n {
		return false
	}
	_, augs, useHash, ok := parseOpts(optional...)
	useAugs := len(augs) != 0
	if !ok || (useAugs && len(augs) != n) {
		return false
	}

	sigFn := func() *P2Affine {
		return sig
	}

	pkFn := func(i uint32, _ *P1Affine) (*P1Affine, []byte) {
		if useAugs {
			return pks[i], augs[i]
		}
		return pks[i], nil
	}

	return coreAggregateVerifyPkInG1(sigFn, sigGroupcheck, pkFn, pksVerify,
		msgs, dst, useHash)
}

// Aggregate verify with compressed signature and public keys
// Uses a dummy signature to get the correct type
func (*P2Affine) AggregateVerifyCompressed(sig []byte, sigGroupcheck bool,
	pks [][]byte, pksVerify bool, msgs []Message, dst []byte,
	optional ...bool) bool { // useHash bool, usePksAsAugs bool

	// sanity checks and argument parsing
	if len(pks) != len(msgs) {
		return false
	}
	useHash := true
	if len(optional) > 0 {
		useHash = optional[0]
	}
	usePksAsAugs := false
	if len(optional) > 1 {
		usePksAsAugs = optional[1]
	}

	sigFn := func() *P2Affine {
		sigP := new(P2Affine)
		if sigP.Uncompress(sig) == nil {
			return nil
		}
		return sigP
	}
	pkFn := func(i uint32, pk *P1Affine) (*P1Affine, []byte) {
		bytes := pks[i]
		if len(bytes) == BLST_P1_SERIALIZE_BYTES && (bytes[0]&0x80) == 0 {
			// Not compressed
			if pk.Deserialize(bytes) == nil {
				return nil, nil
			}
		} else if len(bytes) == BLST_P1_COMPRESS_BYTES && (bytes[0]&0x80) != 0 {
			if pk.Uncompress(bytes) == nil {
				return nil, nil
			}
		} else {
			return nil, nil
		}
		if usePksAsAugs {
			return pk, bytes
		}
		return pk, nil
	}
	return coreAggregateVerifyPkInG1(sigFn, sigGroupcheck, pkFn, pksVerify,
		msgs, dst, useHash)
}

func coreAggregateVerifyPkInG1(sigFn sigGetterP2, sigGroupcheck bool,
	pkFn pkGetterP1, pkValidate bool, msgs []Message, dst []byte,
	optional ...bool) bool { // useHash

	n := len(msgs)
	if n == 0 {
		return false
	}

	useHash := true
	if len(optional) > 0 {
		useHash = optional[0]
	}

	numCores := runtime.GOMAXPROCS(0)
	numThreads := numThreads(n)

	// Each thread will determine next message to process by atomically
	// incrementing curItem, process corresponding pk,msg[,aug] tuple and
	// repeat until n is exceeded.  The resulting accumulations will be
	// fed into the msgsCh channel.
	msgsCh := make(chan Pairing, numThreads)
	valid := int32(1)
	curItem := uint32(0)
	mutex := sync.Mutex{}

	mutex.Lock()
	for tid := 0; tid < numThreads; tid++ {
		go func() {
			pairing := PairingCtx(useHash, dst)
			var temp P1Affine
			for atomic.LoadInt32(&valid) > 0 {
				// Get a work item
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(n) {
					break
				} else if work == 0 && maxProcs == numCores-1 &&
					numThreads == maxProcs {
					// Avoid consuming all cores by waiting until the
					// main thread has completed its miller loop before
					// proceeding.
					mutex.Lock()
					mutex.Unlock() //nolint:staticcheck
				}

				// Pull Public Key and augmentation blob
				curPk, aug := pkFn(work, &temp)
				if curPk == nil {
					atomic.StoreInt32(&valid, 0)
					break
				}

				// Pairing and accumulate
				ret := PairingAggregatePkInG1(pairing, curPk, pkValidate,
					nil, false, msgs[work], aug)
				if ret != ( /*line :701:15*/_Ciconst_BLST_SUCCESS /*line :701:28*/) {
					atomic.StoreInt32(&valid, 0)
					break
				}

				// application might have some async work to do
				runtime.Gosched()
			}
			if atomic.LoadInt32(&valid) > 0 {
				PairingCommit(pairing)
				msgsCh <- pairing
			} else {
				msgsCh <- nil
			}
		}()
	}

	// Uncompress and check signature
	var gtsig Fp12
	sig := sigFn()
	if sig == nil {
		atomic.StoreInt32(&valid, 0)
	}
	if atomic.LoadInt32(&valid) > 0 && sigGroupcheck &&
		!sig.SigValidate(false) {
		atomic.StoreInt32(&valid, 0)
	}
	if atomic.LoadInt32(&valid) > 0 {
		( /*line :729:3*/_Cfunc_blst_aggregated_in_g2 /*line :729:25*/)(&gtsig.cgo, &sig.cgo)
	}
	mutex.Unlock()

	// Accumulate the thread results
	var pairings Pairing
	for i := 0; i < numThreads; i++ {
		msg := <-msgsCh
		if msg != nil {
			if pairings == nil {
				pairings = msg
			} else {
				ret := PairingMerge(pairings, msg)
				if ret != ( /*line :742:15*/_Ciconst_BLST_SUCCESS /*line :742:28*/) {
					atomic.StoreInt32(&valid, 0)
				}
			}
		}
	}
	if atomic.LoadInt32(&valid) == 0 || pairings == nil {
		return false
	}

	return PairingFinalVerify(pairings, &gtsig)
}

func CoreVerifyPkInG1(pk *P1Affine, sig *P2Affine, hash_or_encode bool,
	msg Message, dst []byte, optional ...[]byte) int {

	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	if runtime.NumGoroutine() < maxProcs {
		sigFn := func() *P2Affine {
			return sig
		}
		pkFn := func(_ uint32, _ *P1Affine) (*P1Affine, []byte) {
			return pk, aug
		}
		if !coreAggregateVerifyPkInG1(sigFn, true, pkFn, true, []Message{msg},
			dst, hash_or_encode) {
			return ( /*line :772:11*/_Ciconst_BLST_VERIFY_FAIL /*line :772:28*/)
		}
		return ( /*line :774:10*/_Ciconst_BLST_SUCCESS /*line :774:23*/)
	}

	return int(( /*line :777:13*/_Cfunc_blst_core_verify_pk_in_g1 /*line :777:39*/)(&pk.cgo, &sig.cgo,  /*line :777:60*/_Ctype_bool /*line :777:66*/(hash_or_encode),
		ptrOrNil(msg),  /*line :778:18*/_Ctype_size_t /*line :778:26*/(len(msg)),
		ptrOrNil(dst),  /*line :779:18*/_Ctype_size_t /*line :779:26*/(len(dst)),
		ptrOrNil(aug),  /*line :780:18*/_Ctype_size_t /*line :780:26*/(len(aug))))
}

// pks are assumed to be verified for proof of possession,
// which implies that they are already group-checked
func (sig *P2Affine) FastAggregateVerify(sigGroupcheck bool,
	pks []*P1Affine, msg Message, dst []byte,
	optional ...interface{}) bool { // pass-through to Verify
	n := len(pks)

	// TODO: return value for length zero?
	if n == 0 {
		return false
	}

	aggregator := new(P1Aggregate)
	if !aggregator.Aggregate(pks, false) {
		return false
	}
	pkAff := aggregator.ToAffine()

	// Verify
	return sig.Verify(sigGroupcheck, pkAff, false, msg, dst, optional...)
}

func (*P2Affine) MultipleAggregateVerify(sigs []*P2Affine,
	sigsGroupcheck bool, pks []*P1Affine, pksVerify bool,
	msgs []Message, dst []byte, randFn func(*Scalar), randBits int,
	optional ...interface{}) bool { // useHash

	// Sanity checks and argument parsing
	n := len(pks)
	if n == 0 || len(msgs) != n || len(sigs) != n {
		return false
	}
	_, augs, useHash, ok := parseOpts(optional...)
	useAugs := len(augs) != 0
	if !ok || (useAugs && len(augs) != n) {
		return false
	}

	paramsFn :=
		func(work uint32, _ *P2Affine, _ *P1Affine, rand *Scalar) (
			*P2Affine, *P1Affine, *Scalar, []byte) {
			randFn(rand)
			var aug []byte
			if useAugs {
				aug = augs[work]
			}
			return sigs[work], pks[work], rand, aug
		}

	return multipleAggregateVerifyPkInG1(paramsFn, sigsGroupcheck, pksVerify,
		msgs, dst, randBits, useHash)
}

type mulAggGetterPkInG1 func(work uint32, sig *P2Affine, pk *P1Affine,
	rand *Scalar) (*P2Affine, *P1Affine, *Scalar, []byte)

func multipleAggregateVerifyPkInG1(paramsFn mulAggGetterPkInG1,
	sigsGroupcheck bool, pksVerify bool, msgs []Message,
	dst []byte, randBits int,
	optional ...bool) bool { // useHash
	n := len(msgs)
	if n == 0 {
		return false
	}

	useHash := true
	if len(optional) > 0 {
		useHash = optional[0]
	}

	numThreads := numThreads(n)

	// Each thread will determine next message to process by atomically
	// incrementing curItem, process corresponding pk,msg[,aug] tuple and
	// repeat until n is exceeded.  The resulting accumulations will be
	// fed into the msgsCh channel.
	msgsCh := make(chan Pairing, numThreads)
	valid := int32(1)
	curItem := uint32(0)

	for tid := 0; tid < numThreads; tid++ {
		go func() {
			pairing := PairingCtx(useHash, dst)
			var tempRand Scalar
			var tempPk P1Affine
			var tempSig P2Affine
			for atomic.LoadInt32(&valid) > 0 {
				// Get a work item
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(n) {
					break
				}

				curSig, curPk, curRand, aug := paramsFn(work, &tempSig,
					&tempPk, &tempRand)

				if PairingMulNAggregatePkInG1(pairing, curPk, pksVerify,
					curSig, sigsGroupcheck, curRand,
					randBits, msgs[work], aug) !=
					( /*line :882:6*/_Ciconst_BLST_SUCCESS /*line :882:19*/) {
					atomic.StoreInt32(&valid, 0)
					break
				}

				// application might have some async work to do
				runtime.Gosched()
			}
			if atomic.LoadInt32(&valid) > 0 {
				PairingCommit(pairing)
				msgsCh <- pairing
			} else {
				msgsCh <- nil
			}
		}()
	}

	// Accumulate the thread results
	var pairings Pairing
	for i := 0; i < numThreads; i++ {
		msg := <-msgsCh
		if msg != nil {
			if pairings == nil {
				pairings = msg
			} else {
				ret := PairingMerge(pairings, msg)
				if ret != ( /*line :908:15*/_Ciconst_BLST_SUCCESS /*line :908:28*/) {
					atomic.StoreInt32(&valid, 0)
				}
			}
		}
	}
	if atomic.LoadInt32(&valid) == 0 || pairings == nil {
		return false
	}

	return PairingFinalVerify(pairings, nil)
}

//
// Aggregate P2
//

type aggGetterP2 func(i uint32, temp *P2Affine) *P2Affine
type P2Aggregate struct {
	v *P2
}

// Aggregate uncompressed elements
func (agg *P2Aggregate) Aggregate(elmts []*P2Affine,
	groupcheck bool) bool {
	if len(elmts) == 0 {
		return true
	}
	getter := func(i uint32, _ *P2Affine) *P2Affine { return elmts[i] }
	return agg.coreAggregate(getter, groupcheck, len(elmts))
}

func (agg *P2Aggregate) AggregateWithRandomness(pointsIf interface{},
	scalarsIf interface{}, nbits int, groupcheck bool) bool {
	if groupcheck && !P2AffinesValidate(pointsIf) {
		return false
	}
	agg.v = P2AffinesMult(pointsIf, scalarsIf, nbits)
	return true
}

// Aggregate compressed elements
func (agg *P2Aggregate) AggregateCompressed(elmts [][]byte,
	groupcheck bool) bool {
	if len(elmts) == 0 {
		return true
	}
	getter := func(i uint32, p *P2Affine) *P2Affine {
		bytes := elmts[i]
		if p.Uncompress(bytes) == nil {
			return nil
		}
		return p
	}
	return agg.coreAggregate(getter, groupcheck, len(elmts))
}

func (agg *P2Aggregate) AddAggregate(other *P2Aggregate) {
	if other.v == nil {
		// do nothing
	} else if agg.v == nil {
		agg.v = other.v
	} else {
		( /*line :971:3*/_Cfunc_blst_p2_add_or_double /*line :971:25*/)(&agg.v.cgo, &agg.v.cgo, &other.v.cgo)
	}
}

func (agg *P2Aggregate) Add(elmt *P2Affine, groupcheck bool) bool {
	if groupcheck && !bool(( /*line :976:25*/_Cfunc_blst_p2_affine_in_g2 /*line :976:46*/)(&elmt.cgo)) {
		return false
	}
	if agg.v == nil {
		agg.v = new(P2)
		( /*line :981:3*/_Cfunc_blst_p2_from_affine /*line :981:23*/)(&agg.v.cgo, &elmt.cgo)
	} else {
		( /*line :983:3*/_Cfunc_blst_p2_add_or_double_affine /*line :983:32*/)(&agg.v.cgo, &agg.v.cgo, &elmt.cgo)
	}
	return true
}

func (agg *P2Aggregate) ToAffine() *P2Affine {
	if agg.v == nil {
		return new(P2Affine)
	}
	return agg.v.ToAffine()
}

func (agg *P2Aggregate) coreAggregate(getter aggGetterP2, groupcheck bool,
	n int) bool {

	if n == 0 {
		return true
	}
	// operations are considered short enough for not to care about
	// keeping one core free...
	numThreads := runtime.GOMAXPROCS(0)
	if numThreads > n {
		numThreads = n
	}

	valid := int32(1)
	type result struct {
		agg   *P2
		empty bool
	}
	msgs := make(chan result, numThreads)
	curItem := uint32(0)
	for tid := 0; tid < numThreads; tid++ {
		go func() {
			first := true
			var agg P2
			var temp P2Affine
			for atomic.LoadInt32(&valid) > 0 {
				// Get a work item
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(n) {
					break
				}

				// Signature validate
				curElmt := getter(work, &temp)
				if curElmt == nil {
					atomic.StoreInt32(&valid, 0)
					break
				}
				if groupcheck && !bool(( /*line :1033:28*/_Cfunc_blst_p2_affine_in_g2 /*line :1033:49*/)(&curElmt.cgo)) {
					atomic.StoreInt32(&valid, 0)
					break
				}
				if first {
					( /*line :1038:6*/_Cfunc_blst_p2_from_affine /*line :1038:26*/)(&agg.cgo, &curElmt.cgo)
					first = false
				} else {
					( /*line :1041:6*/_Cfunc_blst_p2_add_or_double_affine /*line :1041:35*/)(&agg.cgo, &agg.cgo, &curElmt.cgo)
				}
				// application might have some async work to do
				runtime.Gosched()
			}
			if first {
				msgs <- result{nil, true}
			} else if atomic.LoadInt32(&valid) > 0 {
				msgs <- result{&agg, false}
			} else {
				msgs <- result{nil, false}
			}
		}()
	}

	// Accumulate the thread results
	first := agg.v == nil
	validLocal := true
	for i := 0; i < numThreads; i++ {
		msg := <-msgs
		if !validLocal || msg.empty {
			// do nothing
		} else if msg.agg == nil {
			validLocal = false
			// This should be unnecessary but seems safer
			atomic.StoreInt32(&valid, 0)
		} else {
			if first {
				agg.v = msg.agg
				first = false
			} else {
				( /*line :1072:5*/_Cfunc_blst_p2_add_or_double /*line :1072:27*/)(&agg.v.cgo, &agg.v.cgo, &msg.agg.cgo)
			}
		}
	}
	if atomic.LoadInt32(&valid) == 0 {
		agg.v = nil
		return false
	}
	return true
}

//
// MIN-SIG
//

//
// PublicKey
//

func (pk *P2Affine) From(s *Scalar) *P2Affine {
	( /*line :1092:2*/_Cfunc_blst_sk_to_pk2_in_g2 /*line :1092:23*/)(nil, &pk.cgo, &s.cgo)
	return pk
}

func (pk *P2Affine) KeyValidate() bool {
	return bool(( /*line :1097:14*/_Cfunc_go_p2_affine_validate /*line :1097:36*/)(&pk.cgo, true))
}

// sigInfcheck, check for infinity, is a way to avoid going
// into resource-consuming verification. Passing 'false' is
// always cryptographically safe, but application might want
// to guard against obviously bogus individual[!] signatures.
func (sig *P1Affine) SigValidate(sigInfcheck bool) bool {
	return bool(( /*line :1105:14*/_Cfunc_go_p1_affine_validate /*line :1105:36*/)(&sig.cgo,  /*line :1105:48*/_Ctype_bool /*line :1105:54*/(sigInfcheck)))
}

//
// Sign
//

func (sig *P1Affine) Sign(sk *SecretKey, msg []byte, dst []byte,
	optional ...interface{}) *P1Affine {
	augSingle, aug, useHash, ok := parseOpts(optional...)
	if !ok || len(aug) != 0 {
		return nil
	}

	var q *P1
	if useHash {
		q = HashToG1(msg, dst, augSingle)
	} else {
		q = EncodeToG1(msg, dst, augSingle)
	}
	( /*line :1125:2*/_Cfunc_blst_sign_pk2_in_g2 /*line :1125:22*/)(nil, &sig.cgo, &q.cgo, &sk.cgo)
	return sig
}

//
// Signature
//

// Functions to return a signature and public key+augmentation tuple.
// This enables point decompression (if needed) to happen in parallel.
type sigGetterP1 func() *P1Affine
type pkGetterP2 func(i uint32, temp *P2Affine) (*P2Affine, []byte)

// Single verify with decompressed pk
func (sig *P1Affine) Verify(sigGroupcheck bool, pk *P2Affine, pkValidate bool,
	msg Message, dst []byte,
	optional ...interface{}) bool { // useHash bool, aug []byte

	aug, _, useHash, ok := parseOpts(optional...)
	if !ok {
		return false
	}
	return sig.AggregateVerify(sigGroupcheck, []*P2Affine{pk}, pkValidate,
		[]Message{msg}, dst, useHash, [][]byte{aug})
}

// Single verify with compressed pk
// Uses a dummy signature to get the correct type
func (dummy *P1Affine) VerifyCompressed(sig []byte, sigGroupcheck bool,
	pk []byte, pkValidate bool, msg Message, dst []byte,
	optional ...bool) bool { // useHash bool, usePksAsAugs bool

	return dummy.AggregateVerifyCompressed(sig, sigGroupcheck,
		[][]byte{pk}, pkValidate,
		[]Message{msg}, dst, optional...)
}

// Aggregate verify with uncompressed signature and public keys
// Note that checking message uniqueness, if required, is left to the user.
// Not all signature schemes require it and this keeps the binding minimal
// and fast. Refer to the Uniq function for one method method of performing
// this check.
func (sig *P1Affine) AggregateVerify(sigGroupcheck bool,
	pks []*P2Affine, pksVerify bool, msgs []Message, dst []byte,
	optional ...interface{}) bool { // useHash bool, augs [][]byte

	// sanity checks and argument parsing
	n := len(pks)
	if n == 0 || len(msgs) != n {
		return false
	}
	_, augs, useHash, ok := parseOpts(optional...)
	useAugs := len(augs) != 0
	if !ok || (useAugs && len(augs) != n) {
		return false
	}

	sigFn := func() *P1Affine {
		return sig
	}

	pkFn := func(i uint32, _ *P2Affine) (*P2Affine, []byte) {
		if useAugs {
			return pks[i], augs[i]
		}
		return pks[i], nil
	}

	return coreAggregateVerifyPkInG2(sigFn, sigGroupcheck, pkFn, pksVerify,
		msgs, dst, useHash)
}

// Aggregate verify with compressed signature and public keys
// Uses a dummy signature to get the correct type
func (*P1Affine) AggregateVerifyCompressed(sig []byte, sigGroupcheck bool,
	pks [][]byte, pksVerify bool, msgs []Message, dst []byte,
	optional ...bool) bool { // useHash bool, usePksAsAugs bool

	// sanity checks and argument parsing
	if len(pks) != len(msgs) {
		return false
	}
	useHash := true
	if len(optional) > 0 {
		useHash = optional[0]
	}
	usePksAsAugs := false
	if len(optional) > 1 {
		usePksAsAugs = optional[1]
	}

	sigFn := func() *P1Affine {
		sigP := new(P1Affine)
		if sigP.Uncompress(sig) == nil {
			return nil
		}
		return sigP
	}
	pkFn := func(i uint32, pk *P2Affine) (*P2Affine, []byte) {
		bytes := pks[i]
		if len(bytes) == BLST_P2_SERIALIZE_BYTES && (bytes[0]&0x80) == 0 {
			// Not compressed
			if pk.Deserialize(bytes) == nil {
				return nil, nil
			}
		} else if len(bytes) == BLST_P2_COMPRESS_BYTES && (bytes[0]&0x80) != 0 {
			if pk.Uncompress(bytes) == nil {
				return nil, nil
			}
		} else {
			return nil, nil
		}
		if usePksAsAugs {
			return pk, bytes
		}
		return pk, nil
	}
	return coreAggregateVerifyPkInG2(sigFn, sigGroupcheck, pkFn, pksVerify,
		msgs, dst, useHash)
}

func coreAggregateVerifyPkInG2(sigFn sigGetterP1, sigGroupcheck bool,
	pkFn pkGetterP2, pkValidate bool, msgs []Message, dst []byte,
	optional ...bool) bool { // useHash

	n := len(msgs)
	if n == 0 {
		return false
	}

	useHash := true
	if len(optional) > 0 {
		useHash = optional[0]
	}

	numCores := runtime.GOMAXPROCS(0)
	numThreads := numThreads(n)

	// Each thread will determine next message to process by atomically
	// incrementing curItem, process corresponding pk,msg[,aug] tuple and
	// repeat until n is exceeded.  The resulting accumulations will be
	// fed into the msgsCh channel.
	msgsCh := make(chan Pairing, numThreads)
	valid := int32(1)
	curItem := uint32(0)
	mutex := sync.Mutex{}

	mutex.Lock()
	for tid := 0; tid < numThreads; tid++ {
		go func() {
			pairing := PairingCtx(useHash, dst)
			var temp P2Affine
			for atomic.LoadInt32(&valid) > 0 {
				// Get a work item
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(n) {
					break
				} else if work == 0 && maxProcs == numCores-1 &&
					numThreads == maxProcs {
					// Avoid consuming all cores by waiting until the
					// main thread has completed its miller loop before
					// proceeding.
					mutex.Lock()
					mutex.Unlock() //nolint:staticcheck
				}

				// Pull Public Key and augmentation blob
				curPk, aug := pkFn(work, &temp)
				if curPk == nil {
					atomic.StoreInt32(&valid, 0)
					break
				}

				// Pairing and accumulate
				ret := PairingAggregatePkInG2(pairing, curPk, pkValidate,
					nil, false, msgs[work], aug)
				if ret != ( /*line :1301:15*/_Ciconst_BLST_SUCCESS /*line :1301:28*/) {
					atomic.StoreInt32(&valid, 0)
					break
				}

				// application might have some async work to do
				runtime.Gosched()
			}
			if atomic.LoadInt32(&valid) > 0 {
				PairingCommit(pairing)
				msgsCh <- pairing
			} else {
				msgsCh <- nil
			}
		}()
	}

	// Uncompress and check signature
	var gtsig Fp12
	sig := sigFn()
	if sig == nil {
		atomic.StoreInt32(&valid, 0)
	}
	if atomic.LoadInt32(&valid) > 0 && sigGroupcheck &&
		!sig.SigValidate(false) {
		atomic.StoreInt32(&valid, 0)
	}
	if atomic.LoadInt32(&valid) > 0 {
		( /*line :1329:3*/_Cfunc_blst_aggregated_in_g1 /*line :1329:25*/)(&gtsig.cgo, &sig.cgo)
	}
	mutex.Unlock()

	// Accumulate the thread results
	var pairings Pairing
	for i := 0; i < numThreads; i++ {
		msg := <-msgsCh
		if msg != nil {
			if pairings == nil {
				pairings = msg
			} else {
				ret := PairingMerge(pairings, msg)
				if ret != ( /*line :1342:15*/_Ciconst_BLST_SUCCESS /*line :1342:28*/) {
					atomic.StoreInt32(&valid, 0)
				}
			}
		}
	}
	if atomic.LoadInt32(&valid) == 0 || pairings == nil {
		return false
	}

	return PairingFinalVerify(pairings, &gtsig)
}

func CoreVerifyPkInG2(pk *P2Affine, sig *P1Affine, hash_or_encode bool,
	msg Message, dst []byte, optional ...[]byte) int {

	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	if runtime.NumGoroutine() < maxProcs {
		sigFn := func() *P1Affine {
			return sig
		}
		pkFn := func(_ uint32, _ *P2Affine) (*P2Affine, []byte) {
			return pk, aug
		}
		if !coreAggregateVerifyPkInG2(sigFn, true, pkFn, true, []Message{msg},
			dst, hash_or_encode) {
			return ( /*line :1372:11*/_Ciconst_BLST_VERIFY_FAIL /*line :1372:28*/)
		}
		return ( /*line :1374:10*/_Ciconst_BLST_SUCCESS /*line :1374:23*/)
	}

	return int(( /*line :1377:13*/_Cfunc_blst_core_verify_pk_in_g2 /*line :1377:39*/)(&pk.cgo, &sig.cgo,  /*line :1377:60*/_Ctype_bool /*line :1377:66*/(hash_or_encode),
		ptrOrNil(msg),  /*line :1378:18*/_Ctype_size_t /*line :1378:26*/(len(msg)),
		ptrOrNil(dst),  /*line :1379:18*/_Ctype_size_t /*line :1379:26*/(len(dst)),
		ptrOrNil(aug),  /*line :1380:18*/_Ctype_size_t /*line :1380:26*/(len(aug))))
}

// pks are assumed to be verified for proof of possession,
// which implies that they are already group-checked
func (sig *P1Affine) FastAggregateVerify(sigGroupcheck bool,
	pks []*P2Affine, msg Message, dst []byte,
	optional ...interface{}) bool { // pass-through to Verify
	n := len(pks)

	// TODO: return value for length zero?
	if n == 0 {
		return false
	}

	aggregator := new(P2Aggregate)
	if !aggregator.Aggregate(pks, false) {
		return false
	}
	pkAff := aggregator.ToAffine()

	// Verify
	return sig.Verify(sigGroupcheck, pkAff, false, msg, dst, optional...)
}

func (*P1Affine) MultipleAggregateVerify(sigs []*P1Affine,
	sigsGroupcheck bool, pks []*P2Affine, pksVerify bool,
	msgs []Message, dst []byte, randFn func(*Scalar), randBits int,
	optional ...interface{}) bool { // useHash

	// Sanity checks and argument parsing
	n := len(pks)
	if n == 0 || len(msgs) != n || len(sigs) != n {
		return false
	}
	_, augs, useHash, ok := parseOpts(optional...)
	useAugs := len(augs) != 0
	if !ok || (useAugs && len(augs) != n) {
		return false
	}

	paramsFn :=
		func(work uint32, _ *P1Affine, _ *P2Affine, rand *Scalar) (
			*P1Affine, *P2Affine, *Scalar, []byte) {
			randFn(rand)
			var aug []byte
			if useAugs {
				aug = augs[work]
			}
			return sigs[work], pks[work], rand, aug
		}

	return multipleAggregateVerifyPkInG2(paramsFn, sigsGroupcheck, pksVerify,
		msgs, dst, randBits, useHash)
}

type mulAggGetterPkInG2 func(work uint32, sig *P1Affine, pk *P2Affine,
	rand *Scalar) (*P1Affine, *P2Affine, *Scalar, []byte)

func multipleAggregateVerifyPkInG2(paramsFn mulAggGetterPkInG2,
	sigsGroupcheck bool, pksVerify bool, msgs []Message,
	dst []byte, randBits int,
	optional ...bool) bool { // useHash
	n := len(msgs)
	if n == 0 {
		return false
	}

	useHash := true
	if len(optional) > 0 {
		useHash = optional[0]
	}

	numThreads := numThreads(n)

	// Each thread will determine next message to process by atomically
	// incrementing curItem, process corresponding pk,msg[,aug] tuple and
	// repeat until n is exceeded.  The resulting accumulations will be
	// fed into the msgsCh channel.
	msgsCh := make(chan Pairing, numThreads)
	valid := int32(1)
	curItem := uint32(0)

	for tid := 0; tid < numThreads; tid++ {
		go func() {
			pairing := PairingCtx(useHash, dst)
			var tempRand Scalar
			var tempPk P2Affine
			var tempSig P1Affine
			for atomic.LoadInt32(&valid) > 0 {
				// Get a work item
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(n) {
					break
				}

				curSig, curPk, curRand, aug := paramsFn(work, &tempSig,
					&tempPk, &tempRand)

				if PairingMulNAggregatePkInG2(pairing, curPk, pksVerify,
					curSig, sigsGroupcheck, curRand,
					randBits, msgs[work], aug) !=
					( /*line :1482:6*/_Ciconst_BLST_SUCCESS /*line :1482:19*/) {
					atomic.StoreInt32(&valid, 0)
					break
				}

				// application might have some async work to do
				runtime.Gosched()
			}
			if atomic.LoadInt32(&valid) > 0 {
				PairingCommit(pairing)
				msgsCh <- pairing
			} else {
				msgsCh <- nil
			}
		}()
	}

	// Accumulate the thread results
	var pairings Pairing
	for i := 0; i < numThreads; i++ {
		msg := <-msgsCh
		if msg != nil {
			if pairings == nil {
				pairings = msg
			} else {
				ret := PairingMerge(pairings, msg)
				if ret != ( /*line :1508:15*/_Ciconst_BLST_SUCCESS /*line :1508:28*/) {
					atomic.StoreInt32(&valid, 0)
				}
			}
		}
	}
	if atomic.LoadInt32(&valid) == 0 || pairings == nil {
		return false
	}

	return PairingFinalVerify(pairings, nil)
}

//
// Aggregate P1
//

type aggGetterP1 func(i uint32, temp *P1Affine) *P1Affine
type P1Aggregate struct {
	v *P1
}

// Aggregate uncompressed elements
func (agg *P1Aggregate) Aggregate(elmts []*P1Affine,
	groupcheck bool) bool {
	if len(elmts) == 0 {
		return true
	}
	getter := func(i uint32, _ *P1Affine) *P1Affine { return elmts[i] }
	return agg.coreAggregate(getter, groupcheck, len(elmts))
}

func (agg *P1Aggregate) AggregateWithRandomness(pointsIf interface{},
	scalarsIf interface{}, nbits int, groupcheck bool) bool {
	if groupcheck && !P1AffinesValidate(pointsIf) {
		return false
	}
	agg.v = P1AffinesMult(pointsIf, scalarsIf, nbits)
	return true
}

// Aggregate compressed elements
func (agg *P1Aggregate) AggregateCompressed(elmts [][]byte,
	groupcheck bool) bool {
	if len(elmts) == 0 {
		return true
	}
	getter := func(i uint32, p *P1Affine) *P1Affine {
		bytes := elmts[i]
		if p.Uncompress(bytes) == nil {
			return nil
		}
		return p
	}
	return agg.coreAggregate(getter, groupcheck, len(elmts))
}

func (agg *P1Aggregate) AddAggregate(other *P1Aggregate) {
	if other.v == nil {
		// do nothing
	} else if agg.v == nil {
		agg.v = other.v
	} else {
		( /*line :1571:3*/_Cfunc_blst_p1_add_or_double /*line :1571:25*/)(&agg.v.cgo, &agg.v.cgo, &other.v.cgo)
	}
}

func (agg *P1Aggregate) Add(elmt *P1Affine, groupcheck bool) bool {
	if groupcheck && !bool(( /*line :1576:25*/_Cfunc_blst_p1_affine_in_g1 /*line :1576:46*/)(&elmt.cgo)) {
		return false
	}
	if agg.v == nil {
		agg.v = new(P1)
		( /*line :1581:3*/_Cfunc_blst_p1_from_affine /*line :1581:23*/)(&agg.v.cgo, &elmt.cgo)
	} else {
		( /*line :1583:3*/_Cfunc_blst_p1_add_or_double_affine /*line :1583:32*/)(&agg.v.cgo, &agg.v.cgo, &elmt.cgo)
	}
	return true
}

func (agg *P1Aggregate) ToAffine() *P1Affine {
	if agg.v == nil {
		return new(P1Affine)
	}
	return agg.v.ToAffine()
}

func (agg *P1Aggregate) coreAggregate(getter aggGetterP1, groupcheck bool,
	n int) bool {

	if n == 0 {
		return true
	}
	// operations are considered short enough for not to care about
	// keeping one core free...
	numThreads := runtime.GOMAXPROCS(0)
	if numThreads > n {
		numThreads = n
	}

	valid := int32(1)
	type result struct {
		agg   *P1
		empty bool
	}
	msgs := make(chan result, numThreads)
	curItem := uint32(0)
	for tid := 0; tid < numThreads; tid++ {
		go func() {
			first := true
			var agg P1
			var temp P1Affine
			for atomic.LoadInt32(&valid) > 0 {
				// Get a work item
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(n) {
					break
				}

				// Signature validate
				curElmt := getter(work, &temp)
				if curElmt == nil {
					atomic.StoreInt32(&valid, 0)
					break
				}
				if groupcheck && !bool(( /*line :1633:28*/_Cfunc_blst_p1_affine_in_g1 /*line :1633:49*/)(&curElmt.cgo)) {
					atomic.StoreInt32(&valid, 0)
					break
				}
				if first {
					( /*line :1638:6*/_Cfunc_blst_p1_from_affine /*line :1638:26*/)(&agg.cgo, &curElmt.cgo)
					first = false
				} else {
					( /*line :1641:6*/_Cfunc_blst_p1_add_or_double_affine /*line :1641:35*/)(&agg.cgo, &agg.cgo, &curElmt.cgo)
				}
				// application might have some async work to do
				runtime.Gosched()
			}
			if first {
				msgs <- result{nil, true}
			} else if atomic.LoadInt32(&valid) > 0 {
				msgs <- result{&agg, false}
			} else {
				msgs <- result{nil, false}
			}
		}()
	}

	// Accumulate the thread results
	first := agg.v == nil
	validLocal := true
	for i := 0; i < numThreads; i++ {
		msg := <-msgs
		if !validLocal || msg.empty {
			// do nothing
		} else if msg.agg == nil {
			validLocal = false
			// This should be unnecessary but seems safer
			atomic.StoreInt32(&valid, 0)
		} else {
			if first {
				agg.v = msg.agg
				first = false
			} else {
				( /*line :1672:5*/_Cfunc_blst_p1_add_or_double /*line :1672:27*/)(&agg.v.cgo, &agg.v.cgo, &msg.agg.cgo)
			}
		}
	}
	if atomic.LoadInt32(&valid) == 0 {
		agg.v = nil
		return false
	}
	return true
}
func PairingAggregatePkInG1(ctx Pairing, PK *P1Affine, pkValidate bool,
	sig *P2Affine, sigGroupcheck bool, msg []byte,
	optional ...[]byte) int { // aug
	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	r := ( /*line :1690:7*/_Cfunc_blst_pairing_chk_n_aggr_pk_in_g1 /*line :1690:40*/)(&ctx[0],
		PK.asPtr(),  /*line :1691:15*/_Ctype_bool /*line :1691:21*/(pkValidate),
		sig.asPtr(),  /*line :1692:16*/_Ctype_bool /*line :1692:22*/(sigGroupcheck),
		ptrOrNil(msg),  /*line :1693:18*/_Ctype_size_t /*line :1693:26*/(len(msg)),
		ptrOrNil(aug),  /*line :1694:18*/_Ctype_size_t /*line :1694:26*/(len(aug)))

	return int(r)
}

func PairingMulNAggregatePkInG1(ctx Pairing, PK *P1Affine, pkValidate bool,
	sig *P2Affine, sigGroupcheck bool,
	rand *Scalar, randBits int, msg []byte,
	optional ...[]byte) int { // aug
	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	r := ( /*line :1708:7*/_Cfunc_blst_pairing_chk_n_mul_n_aggr_pk_in_g1 /*line :1708:46*/)(&ctx[0],
		PK.asPtr(),  /*line :1709:15*/_Ctype_bool /*line :1709:21*/(pkValidate),
		sig.asPtr(),  /*line :1710:16*/_Ctype_bool /*line :1710:22*/(sigGroupcheck),
		&rand.cgo.b[0],  /*line :1711:19*/_Ctype_size_t /*line :1711:27*/(randBits),
		ptrOrNil(msg),  /*line :1712:18*/_Ctype_size_t /*line :1712:26*/(len(msg)),
		ptrOrNil(aug),  /*line :1713:18*/_Ctype_size_t /*line :1713:26*/(len(aug)))

	return int(r)
}

//
// Serialization/Deserialization.
//

// P1 Serdes
func (p1 *P1Affine) Serialize() []byte {
	var out [BLST_P1_SERIALIZE_BYTES]byte
	( /*line :1725:2*/_Cfunc_blst_p1_affine_serialize /*line :1725:27*/)((* /*line :1725:31*/_Ctype_byte /*line :1725:37*/)(&out[0]), &p1.cgo)
	return out[:]
}

func (p1 *P1Affine) Deserialize(in []byte) *P1Affine {
	if len(in) != BLST_P1_SERIALIZE_BYTES {
		return nil
	}
	if ( /*line :1733:5*/_Cfunc_blst_p1_deserialize /*line :1733:25*/)(&p1.cgo, (* /*line :1733:38*/_Ctype_byte /*line :1733:44*/)(&in[0])) != ( /*line :1733:58*/_Ciconst_BLST_SUCCESS /*line :1733:71*/) {
		return nil
	}
	return p1
}
func (p1 *P1Affine) Compress() []byte {
	var out [BLST_P1_COMPRESS_BYTES]byte
	( /*line :1740:2*/_Cfunc_blst_p1_affine_compress /*line :1740:26*/)((* /*line :1740:30*/_Ctype_byte /*line :1740:36*/)(&out[0]), &p1.cgo)
	return out[:]
}

func (p1 *P1Affine) Uncompress(in []byte) *P1Affine {
	if len(in) != BLST_P1_COMPRESS_BYTES {
		return nil
	}
	if ( /*line :1748:5*/_Cfunc_blst_p1_uncompress /*line :1748:24*/)(&p1.cgo, (* /*line :1748:37*/_Ctype_byte /*line :1748:43*/)(&in[0])) != ( /*line :1748:57*/_Ciconst_BLST_SUCCESS /*line :1748:70*/) {
		return nil
	}
	return p1
}

func (p1 *P1Affine) InG1() bool {
	return bool(( /*line :1755:14*/_Cfunc_blst_p1_affine_in_g1 /*line :1755:35*/)(&p1.cgo))
}

func (*P1Affine) BatchUncompress(in [][]byte) []*P1Affine {
	// Allocate space for all of the resulting points. Later we'll save pointers
	// and return those so that the result could be used in other functions,
	// such as MultipleAggregateVerify.
	n := len(in)
	points := make([]P1Affine, n)
	pointsPtrs := make([]*P1Affine, n)

	numThreads := numThreads(n)

	// Each thread will determine next message to process by atomically
	// incrementing curItem, process corresponding point, and
	// repeat until n is exceeded. Each thread will send a result (true for
	// success, false for failure) into the channel when complete.
	resCh := make(chan bool, numThreads)
	valid := int32(1)
	curItem := uint32(0)
	for tid := 0; tid < numThreads; tid++ {
		go func() {
			for atomic.LoadInt32(&valid) > 0 {
				// Get a work item
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(n) {
					break
				}
				if points[work].Uncompress(in[work]) == nil {
					atomic.StoreInt32(&valid, 0)
					break
				}
				pointsPtrs[work] = &points[work]
			}
			if atomic.LoadInt32(&valid) > 0 {
				resCh <- true
			} else {
				resCh <- false
			}
		}()
	}

	// Collect the threads
	result := true
	for i := 0; i < numThreads; i++ {
		if !<-resCh {
			result = false
		}
	}
	if atomic.LoadInt32(&valid) == 0 || !result {
		return nil
	}
	return pointsPtrs
}

func (p1 *P1) Serialize() []byte {
	var out [BLST_P1_SERIALIZE_BYTES]byte
	( /*line :1812:2*/_Cfunc_blst_p1_serialize /*line :1812:20*/)((* /*line :1812:24*/_Ctype_byte /*line :1812:30*/)(&out[0]), &p1.cgo)
	return out[:]
}
func (p1 *P1) Compress() []byte {
	var out [BLST_P1_COMPRESS_BYTES]byte
	( /*line :1817:2*/_Cfunc_blst_p1_compress /*line :1817:19*/)((* /*line :1817:23*/_Ctype_byte /*line :1817:29*/)(&out[0]), &p1.cgo)
	return out[:]
}

func (p1 *P1) MultAssign(scalarIf interface{}, optional ...int) *P1 {
	var nbits int
	var scalar * /*line :1823:14*/_Ctype_byte /*line :1823:20*/
	switch val := scalarIf.(type) {
	case []byte:
		scalar = (* /*line :1826:14*/_Ctype_byte /*line :1826:20*/)(&val[0])
		nbits = len(val) * 8
	case *Scalar:
		scalar = &val.cgo.b[0]
		nbits = 255
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}
	if len(optional) > 0 {
		nbits = optional[0]
	}
	( /*line :1837:2*/_Cfunc_blst_p1_mult /*line :1837:15*/)(&p1.cgo, &p1.cgo, scalar,  /*line :1837:43*/_Ctype_size_t /*line :1837:51*/(nbits))
	return p1
}

func (p1 *P1) Mult(scalarIf interface{}, optional ...int) *P1 {
	ret := *p1
	return ret.MultAssign(scalarIf, optional...)
}

func (p1 *P1) AddAssign(pointIf interface{}) *P1 {
	switch val := pointIf.(type) {
	case *P1:
		( /*line :1849:3*/_Cfunc_blst_p1_add_or_double /*line :1849:25*/)(&p1.cgo, &p1.cgo, &val.cgo)
	case *P1Affine:
		( /*line :1851:3*/_Cfunc_blst_p1_add_or_double_affine /*line :1851:32*/)(&p1.cgo, &p1.cgo, &val.cgo)
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}
	return p1
}

func (p1 *P1) Add(pointIf interface{}) *P1 {
	ret := *p1
	return ret.AddAssign(pointIf)
}

func (p1 *P1) SubAssign(pointIf interface{}) *P1 {
	var x * /*line :1864:9*/_Ctype_blst_fp /*line :1864:18*/
	var affine  /*line :1865:13*/_Ctype_bool /*line :1865:19*/
	switch val := pointIf.(type) {
	case *P1:
		x = &val.cgo.x
		affine = false
	case *P1Affine:
		x = &val.cgo.x
		affine = true
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}
	( /*line :1876:2*/_Cfunc_go_p1_sub_assign /*line :1876:19*/)(&p1.cgo, x, affine)
	return p1
}

func (p1 *P1) Sub(pointIf interface{}) *P1 {
	ret := *p1
	return ret.SubAssign(pointIf)
}

func P1Generator() *P1 {
	return &cgo_p1Generator
}

// 'acc += point * scalar', passing 'nil' for 'point' means "use the
//
//	group generator point"
func (acc *P1) MultNAccumulate(pointIf interface{}, scalarIf interface{},
	optional ...int) *P1 {
	var x * /*line :1894:9*/_Ctype_blst_fp /*line :1894:18*/
	var affine  /*line :1895:13*/_Ctype_bool /*line :1895:19*/
	if pointIf != nil {
		switch val := pointIf.(type) {
		case *P1:
			x = &val.cgo.x
			affine = false
		case *P1Affine:
			x = &val.cgo.x
			affine = true
		default:
			panic(fmt.Sprintf("unsupported type %T", val))
		}
	}
	var nbits int
	var scalar * /*line :1909:14*/_Ctype_byte /*line :1909:20*/
	switch val := scalarIf.(type) {
	case []byte:
		scalar = (* /*line :1912:14*/_Ctype_byte /*line :1912:20*/)(&val[0])
		nbits = len(val) * 8
	case *Scalar:
		scalar = &val.cgo.b[0]
		nbits = 255
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}
	if len(optional) > 0 {
		nbits = optional[0]
	}
	( /*line :1923:2*/_Cfunc_go_p1_mult_n_acc /*line :1923:19*/)(&acc.cgo, x, affine, scalar,  /*line :1923:50*/_Ctype_size_t /*line :1923:58*/(nbits))
	return acc
}

//
// Affine
//

func (p *P1) ToAffine() *P1Affine {
	var pa P1Affine
	( /*line :1933:2*/_Cfunc_blst_p1_to_affine /*line :1933:20*/)(&pa.cgo, &p.cgo)
	return &pa
}

func (p *P1) FromAffine(pa *P1Affine) {
	( /*line :1938:2*/_Cfunc_blst_p1_from_affine /*line :1938:22*/)(&p.cgo, &pa.cgo)
}

// Hash
func HashToG1(msg []byte, dst []byte,
	optional ...[]byte) *P1 { // aug
	var q P1

	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	( /*line :1951:2*/_Cfunc_blst_hash_to_g1 /*line :1951:18*/)(&q.cgo, ptrOrNil(msg),  /*line :1951:43*/_Ctype_size_t /*line :1951:51*/(len(msg)),
		ptrOrNil(dst),  /*line :1952:18*/_Ctype_size_t /*line :1952:26*/(len(dst)),
		ptrOrNil(aug),  /*line :1953:18*/_Ctype_size_t /*line :1953:26*/(len(aug)))
	return &q
}

func EncodeToG1(msg []byte, dst []byte,
	optional ...[]byte) *P1 { // aug
	var q P1

	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	( /*line :1966:2*/_Cfunc_blst_encode_to_g1 /*line :1966:20*/)(&q.cgo, ptrOrNil(msg),  /*line :1966:45*/_Ctype_size_t /*line :1966:53*/(len(msg)),
		ptrOrNil(dst),  /*line :1967:18*/_Ctype_size_t /*line :1967:26*/(len(dst)),
		ptrOrNil(aug),  /*line :1968:18*/_Ctype_size_t /*line :1968:26*/(len(aug)))
	return &q
}

//
// Multi-point/scalar operations
//

func P1sToAffine(points []*P1, optional ...int) P1Affines {
	var npoints int
	if len(optional) > 0 {
		npoints = optional[0]
	} else {
		npoints = len(points)
	}
	ret := make([]P1Affine, npoints)
	_cgoCheckPointer := func(...interface{}) {}
	func() { var _cgo0 *_Ctype_struct___4 = /*line :1985:23*/&ret[0].cgo; _cgoIndex1 := &/*line :1985:66*/points; _cgo1 := /*line :1985:36*/(**_Ctype_blst_p1 /*line :1985:48*/)(unsafe.Pointer(&(*_cgoIndex1)[0])); var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :1986:11*/(npoints); _cgoCheckPointer(_cgo1, *_cgoIndex1); /*line :1986:21*/_Cfunc_blst_p1s_to_affine(_cgo0, _cgo1, _cgo2); }()
	return ret
}

func (points P1s) ToAffine(optional ...P1Affines) P1Affines {
	npoints := len(points)
	var ret P1Affines

	if len(optional) > 0 { // used in benchmark
		ret = optional[0]
		if len(ret) < npoints {
			panic("npoints mismatch")
		}
	} else {
		ret = make([]P1Affine, npoints)
	}

	if maxProcs < 2 || npoints < 768 {
		( /*line :2004:3*/_Cfunc_go_p1slice_to_affine /*line :2004:24*/)(&ret[0].cgo, &points[0].cgo,  /*line :2004:55*/_Ctype_size_t /*line :2004:63*/(npoints))
		return ret
	}

	nslices := (npoints + 511) / 512
	if nslices > maxProcs {
		nslices = maxProcs
	}
	delta, rem := npoints/nslices+1, npoints%nslices

	var wg sync.WaitGroup
	wg.Add(nslices)
	for x := 0; x < npoints; x += delta {
		if rem == 0 {
			delta -= 1
		}
		rem -= 1
		go func(out *P1Affine, inp *P1, delta int) {
			( /*line :2022:4*/_Cfunc_go_p1slice_to_affine /*line :2022:25*/)(&out.cgo, &inp.cgo,  /*line :2022:47*/_Ctype_size_t /*line :2022:55*/(delta))
			wg.Done()
		}(&ret[x], &points[x], delta)
	}
	wg.Wait()

	return ret
}

//
// Batch addition
//

func P1AffinesAdd(points []*P1Affine, optional ...int) *P1 {
	var npoints int
	if len(optional) > 0 {
		npoints = optional[0]
	} else {
		npoints = len(points)
	}
	var ret P1
	_cgoCheckPointer := func(...interface{}) {}
	func() { var _cgo0 *_Ctype_struct___7 = /*line :2044:17*/&ret.cgo; _cgoIndex1 := &/*line :2044:64*/points; _cgo1 := /*line :2044:27*/(**_Ctype_blst_p1_affine /*line :2044:46*/)(unsafe.Pointer(&(*_cgoIndex1)[0])); var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :2045:11*/(npoints); _cgoCheckPointer(_cgo1, *_cgoIndex1); /*line :2045:21*/_Cfunc_blst_p1s_add(_cgo0, _cgo1, _cgo2); }()
	return &ret
}

func (points P1Affines) Add() *P1 {
	npoints := len(points)
	if maxProcs < 2 || npoints < 768 {
		var ret P1
		( /*line :2053:3*/_Cfunc_go_p1slice_add /*line :2053:18*/)(&ret.cgo, &points[0].cgo,  /*line :2053:46*/_Ctype_size_t /*line :2053:54*/(npoints))
		return &ret
	}

	nslices := (npoints + 511) / 512
	if nslices > maxProcs {
		nslices = maxProcs
	}
	delta, rem := npoints/nslices+1, npoints%nslices

	msgs := make(chan P1, nslices)
	for x := 0; x < npoints; x += delta {
		if rem == 0 {
			delta -= 1
		}
		rem -= 1
		go func(points *P1Affine, delta int) {
			var ret P1
			( /*line :2071:4*/_Cfunc_go_p1slice_add /*line :2071:19*/)(&ret.cgo, &points.cgo,  /*line :2071:44*/_Ctype_size_t /*line :2071:52*/(delta))
			msgs <- ret
		}(&points[x], delta)
	}

	ret := <-msgs
	for i := 1; i < nslices; i++ {
		msg := <-msgs
		( /*line :2079:3*/_Cfunc_blst_p1_add_or_double /*line :2079:25*/)(&ret.cgo, &ret.cgo, &msg.cgo)
	}
	return &ret
}

func (points P1s) Add() *P1 {
	return points.ToAffine().Add()
}

//
// Multi-scalar multiplication
//

func P1AffinesMult(pointsIf interface{}, scalarsIf interface{}, nbits int) *P1 {
	var npoints int
	switch val := pointsIf.(type) {
	case []*P1Affine:
		npoints = len(val)
	case []P1Affine:
		npoints = len(val)
	case P1Affines:
		npoints = len(val)
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}

	nbytes := (nbits + 7) / 8
	var scalars []* /*line :2106:17*/_Ctype_byte /*line :2106:23*/
	switch val := scalarsIf.(type) {
	case []byte:
		if len(val) < npoints*nbytes {
			return nil
		}
	case [][]byte:
		if len(val) < npoints {
			return nil
		}
		scalars = make([]* /*line :2116:21*/_Ctype_byte /*line :2116:27*/, npoints)
		for i := range scalars {
			scalars[i] = (* /*line :2118:19*/_Ctype_byte /*line :2118:25*/)(&val[i][0])
		}
	case []Scalar:
		if len(val) < npoints {
			return nil
		}
		if nbits <= 248 {
			scalars = make([]* /*line :2125:22*/_Ctype_byte /*line :2125:28*/, npoints)
			for i := range scalars {
				scalars[i] = &val[i].cgo.b[0]
			}
		}
	case []*Scalar:
		if len(val) < npoints {
			return nil
		}
		scalars = make([]* /*line :2134:21*/_Ctype_byte /*line :2134:27*/, npoints)
		for i := range scalars {
			scalars[i] = &val[i].cgo.b[0]
		}
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}

	numThreads := numThreads(0)

	if numThreads < 2 {
		sz := int(( /*line :2145:13*/_Cfunc_blst_p1s_mult_pippenger_scratch_sizeof /*line :2145:52*/)( /*line :2145:54*/_Ctype_size_t /*line :2145:62*/(npoints))) / 8
		scratch := make([]uint64, sz)

		pointsBySlice := [2]* /*line :2148:24*/_Ctype_blst_p1_affine /*line :2148:40*/{nil, nil}
		var p_points ** /*line :2149:18*/_Ctype_blst_p1_affine /*line :2149:34*/
		switch val := pointsIf.(type) {
		case []*P1Affine:
			p_points = (** /*line :2152:18*/_Ctype_blst_p1_affine /*line :2152:34*/)(unsafe.Pointer(&val[0]))
		case []P1Affine:
			pointsBySlice[0] = &val[0].cgo
			p_points = &pointsBySlice[0]
		case P1Affines:
			pointsBySlice[0] = &val[0].cgo
			p_points = &pointsBySlice[0]
		}

		scalarsBySlice := [2]* /*line :2161:25*/_Ctype_byte /*line :2161:31*/{nil, nil}
		var p_scalars ** /*line :2162:19*/_Ctype_byte /*line :2162:25*/
		switch val := scalarsIf.(type) {
		case []byte:
			scalarsBySlice[0] = (* /*line :2165:26*/_Ctype_byte /*line :2165:32*/)(&val[0])
			p_scalars = &scalarsBySlice[0]
		case [][]byte:
			p_scalars = &scalars[0]
		case []Scalar:
			if nbits > 248 {
				scalarsBySlice[0] = &val[0].cgo.b[0]
				p_scalars = &scalarsBySlice[0]
			} else {
				p_scalars = &scalars[0]
			}
		case []*Scalar:
			p_scalars = &scalars[0]
		}

		var ret P1
		_cgoCheckPointer := func(...interface{}) {}
		func() { var _cgo0 *_Ctype_struct___7 = /*line :2182:29*/&ret.cgo; _cgo1 := /*line :2182:39*/p_points; var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :2182:57*/(npoints); _cgo3 := /*line :2183:4*/p_scalars; var _cgo4 _Ctype_size_t = _Ctype_size_t /*line :2183:23*/(nbits); var _cgo5 *_Ctype_limb_t = /*line :2184:4*/(*_Ctype_limb_t /*line :2184:14*/)(&scratch[0]); _cgoCheckPointer(_cgo1, nil); _cgoCheckPointer(_cgo3, nil); /*line :2184:29*/_Cfunc_blst_p1s_mult_pippenger(_cgo0, _cgo1, _cgo2, _cgo3, _cgo4, _cgo5); }()

		for i := range scalars {
			scalars[i] = nil
		}

		return &ret
	}

	if npoints < 32 {
		if numThreads > npoints {
			numThreads = npoints
		}

		curItem := uint32(0)
		msgs := make(chan P1, numThreads)

		for tid := 0; tid < numThreads; tid++ {
			go func() {
				var acc P1

				for {
					workItem := int(atomic.AddUint32(&curItem, 1) - 1)
					if workItem >= npoints {
						break
					}

					var point *P1Affine
					switch val := pointsIf.(type) {
					case []*P1Affine:
						point = val[workItem]
					case []P1Affine:
						point = &val[workItem]
					case P1Affines:
						point = &val[workItem]
					}

					var scalar * /*line :2221:18*/_Ctype_byte /*line :2221:24*/
					switch val := scalarsIf.(type) {
					case []byte:
						scalar = (* /*line :2224:18*/_Ctype_byte /*line :2224:24*/)(&val[workItem*nbytes])
					case [][]byte:
						scalar = scalars[workItem]
					case []Scalar:
						if nbits > 248 {
							scalar = &val[workItem].cgo.b[0]
						} else {
							scalar = scalars[workItem]
						}
					case []*Scalar:
						scalar = scalars[workItem]
					}

					( /*line :2237:6*/_Cfunc_go_p1_mult_n_acc /*line :2237:23*/)(&acc.cgo, &point.cgo.x, true,
						scalar,  /*line :2238:15*/_Ctype_size_t /*line :2238:23*/(nbits))
				}

				msgs <- acc
			}()
		}

		ret := <-msgs
		for tid := 1; tid < numThreads; tid++ {
			point := <-msgs
			( /*line :2248:4*/_Cfunc_blst_p1_add_or_double /*line :2248:26*/)(&ret.cgo, &ret.cgo, &point.cgo)
		}

		for i := range scalars {
			scalars[i] = nil
		}

		return &ret
	}

	// this is sizeof(scratch[0])
	sz := int(( /*line :2259:12*/_Cfunc_blst_p1s_mult_pippenger_scratch_sizeof /*line :2259:51*/)(0)) / 8

	nx, ny, window := breakdown(nbits, pippenger_window_size(npoints),
		numThreads)

	// |grid[]| holds "coordinates" and place for result
	grid := make([]struct {
		x, dx, y, dy int
		point        P1
	}, nx*ny)

	dx := npoints / nx
	y := window * (ny - 1)
	total := 0
	for ; total < nx; total++ {
		grid[total].x = total * dx
		grid[total].dx = dx
		grid[total].y = y
		grid[total].dy = nbits - y
	}
	grid[total-1].dx = npoints - grid[total-1].x

	for y > 0 {
		y -= window
		for i := 0; i < nx; i++ {
			grid[total].x = grid[i].x
			grid[total].dx = grid[i].dx
			grid[total].y = y
			grid[total].dy = window
			total++
		}
	}

	if numThreads > total {
		numThreads = total
	}

	msgsCh := make(chan int, ny)
	rowSync := make([]int32, ny) // count up to |nx|
	curItem := int32(0)
	for tid := 0; tid < numThreads; tid++ {
		go func() {
			scratch := make([]uint64, sz<<uint(window-1))
			pointsBySlice := [2]* /*line :2302:25*/_Ctype_blst_p1_affine /*line :2302:41*/{nil, nil}
			scalarsBySlice := [2]* /*line :2303:26*/_Ctype_byte /*line :2303:32*/{nil, nil}
			_cgoCheckPointer := func(...interface{}) {}

			for {
				workItem := atomic.AddInt32(&curItem, 1) - 1
				if int(workItem) >= total {
					break
				}

				x := grid[workItem].x
				y := grid[workItem].y

				var p_points ** /*line :2315:20*/_Ctype_blst_p1_affine /*line :2315:36*/
				switch val := pointsIf.(type) {
				case []*P1Affine:
					p_points = (** /*line :2318:20*/_Ctype_blst_p1_affine /*line :2318:36*/)(unsafe.Pointer(&val[x]))
				case []P1Affine:
					pointsBySlice[0] = &val[x].cgo
					p_points = &pointsBySlice[0]
				case P1Affines:
					pointsBySlice[0] = &val[x].cgo
					p_points = &pointsBySlice[0]
				}

				var p_scalars ** /*line :2327:21*/_Ctype_byte /*line :2327:27*/
				switch val := scalarsIf.(type) {
				case []byte:
					scalarsBySlice[0] = (* /*line :2330:28*/_Ctype_byte /*line :2330:34*/)(&val[x*nbytes])
					p_scalars = &scalarsBySlice[0]
				case [][]byte:
					p_scalars = &scalars[x]
				case []Scalar:
					if nbits > 248 {
						scalarsBySlice[0] = &val[x].cgo.b[0]
						p_scalars = &scalarsBySlice[0]
					} else {
						p_scalars = &scalars[x]
					}
				case []*Scalar:
					p_scalars = &scalars[x]
				}

				func() { var _cgo0 *_Ctype_struct___7 = /*line :2345:31*/&grid[workItem].point.cgo; _cgo1 := /*line :2346:6*/p_points; var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :2346:24*/(grid[workItem].dx); _cgo3 := /*line :2347:6*/p_scalars; var _cgo4 _Ctype_size_t = _Ctype_size_t /*line :2347:25*/(nbits); var _cgo5 *_Ctype_limb_t = /*line :2348:6*/(*_Ctype_limb_t /*line :2348:16*/)(&scratch[0]); var _cgo6 _Ctype_size_t = _Ctype_size_t /*line :2349:14*/(y); var _cgo7 _Ctype_size_t = _Ctype_size_t /*line :2349:27*/(window); _cgoCheckPointer(_cgo1, nil); _cgoCheckPointer(_cgo3, nil); /*line :2349:36*/_Cfunc_blst_p1s_tile_pippenger(_cgo0, _cgo1, _cgo2, _cgo3, _cgo4, _cgo5, _cgo6, _cgo7); }()

				if atomic.AddInt32(&rowSync[y/window], 1) == int32(nx) {
					msgsCh <- y // "row" is done
				} else {
					runtime.Gosched() // be nice to the application
				}
			}

			pointsBySlice[0] = nil
			scalarsBySlice[0] = nil
		}()
	}

	var ret P1
	rows := make([]bool, ny)
	row := 0                  // actually index in |grid[]|
	for i := 0; i < ny; i++ { // we expect |ny| messages, one per "row"
		y := <-msgsCh
		rows[y/window] = true  // mark the "row"
		for grid[row].y == y { // if it's current "row", process it
			for row < total && grid[row].y == y {
				( /*line :2371:5*/_Cfunc_blst_p1_add_or_double /*line :2371:27*/)(&ret.cgo, &ret.cgo, &grid[row].point.cgo)
				row++
			}
			if y == 0 {
				break // one can as well 'return &ret' here
			}
			for j := 0; j < window; j++ {
				( /*line :2378:5*/_Cfunc_blst_p1_double /*line :2378:20*/)(&ret.cgo, &ret.cgo)
			}
			y -= window
			if !rows[y/window] { // see if next "row" was marked already
				break
			}
		}
	}

	for i := range scalars {
		scalars[i] = nil
	}

	return &ret
}

func (points P1Affines) Mult(scalarsIf interface{}, nbits int) *P1 {
	return P1AffinesMult(points, scalarsIf, nbits)
}

func (points P1s) Mult(scalarsIf interface{}, nbits int) *P1 {
	return points.ToAffine().Mult(scalarsIf, nbits)
}

//
// Group-check
//

func P1AffinesValidate(pointsIf interface{}) bool {
	var npoints int
	switch val := pointsIf.(type) {
	case []*P1Affine:
		npoints = len(val)
	case []P1Affine:
		npoints = len(val)
	case P1Affines:
		npoints = len(val)
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}

	numThreads := numThreads(npoints)

	if numThreads < 2 {
		for i := 0; i < npoints; i++ {
			var point *P1Affine

			switch val := pointsIf.(type) {
			case []*P1Affine:
				point = val[i]
			case []P1Affine:
				point = &val[i]
			case P1Affines:
				point = &val[i]
			default:
				panic(fmt.Sprintf("unsupported type %T", val))
			}

			if !( /*line :2436:8*/_Cfunc_go_p1_affine_validate /*line :2436:30*/)(&point.cgo, true) {
				return false
			}
		}

		return true
	}

	valid := int32(1)
	curItem := uint32(0)

	var wg sync.WaitGroup
	wg.Add(numThreads)

	for tid := 0; tid < numThreads; tid++ {
		go func() {
			for atomic.LoadInt32(&valid) != 0 {
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(npoints) {
					break
				}

				var point *P1Affine

				switch val := pointsIf.(type) {
				case []*P1Affine:
					point = val[work]
				case []P1Affine:
					point = &val[work]
				case P1Affines:
					point = &val[work]
				default:
					panic(fmt.Sprintf("unsupported type %T", val))
				}

				if !( /*line :2471:9*/_Cfunc_go_p1_affine_validate /*line :2471:31*/)(&point.cgo, true) {
					atomic.StoreInt32(&valid, 0)
					break
				}
			}

			wg.Done()
		}()
	}

	wg.Wait()

	return atomic.LoadInt32(&valid) != 0
}

func (points P1Affines) Validate() bool {
	return P1AffinesValidate(points)
}
func PairingAggregatePkInG2(ctx Pairing, PK *P2Affine, pkValidate bool,
	sig *P1Affine, sigGroupcheck bool, msg []byte,
	optional ...[]byte) int { // aug
	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	r := ( /*line :2497:7*/_Cfunc_blst_pairing_chk_n_aggr_pk_in_g2 /*line :2497:40*/)(&ctx[0],
		PK.asPtr(),  /*line :2498:15*/_Ctype_bool /*line :2498:21*/(pkValidate),
		sig.asPtr(),  /*line :2499:16*/_Ctype_bool /*line :2499:22*/(sigGroupcheck),
		ptrOrNil(msg),  /*line :2500:18*/_Ctype_size_t /*line :2500:26*/(len(msg)),
		ptrOrNil(aug),  /*line :2501:18*/_Ctype_size_t /*line :2501:26*/(len(aug)))

	return int(r)
}

func PairingMulNAggregatePkInG2(ctx Pairing, PK *P2Affine, pkValidate bool,
	sig *P1Affine, sigGroupcheck bool,
	rand *Scalar, randBits int, msg []byte,
	optional ...[]byte) int { // aug
	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	r := ( /*line :2515:7*/_Cfunc_blst_pairing_chk_n_mul_n_aggr_pk_in_g2 /*line :2515:46*/)(&ctx[0],
		PK.asPtr(),  /*line :2516:15*/_Ctype_bool /*line :2516:21*/(pkValidate),
		sig.asPtr(),  /*line :2517:16*/_Ctype_bool /*line :2517:22*/(sigGroupcheck),
		&rand.cgo.b[0],  /*line :2518:19*/_Ctype_size_t /*line :2518:27*/(randBits),
		ptrOrNil(msg),  /*line :2519:18*/_Ctype_size_t /*line :2519:26*/(len(msg)),
		ptrOrNil(aug),  /*line :2520:18*/_Ctype_size_t /*line :2520:26*/(len(aug)))

	return int(r)
}

//
// Serialization/Deserialization.
//

// P2 Serdes
func (p2 *P2Affine) Serialize() []byte {
	var out [BLST_P2_SERIALIZE_BYTES]byte
	( /*line :2532:2*/_Cfunc_blst_p2_affine_serialize /*line :2532:27*/)((* /*line :2532:31*/_Ctype_byte /*line :2532:37*/)(&out[0]), &p2.cgo)
	return out[:]
}

func (p2 *P2Affine) Deserialize(in []byte) *P2Affine {
	if len(in) != BLST_P2_SERIALIZE_BYTES {
		return nil
	}
	if ( /*line :2540:5*/_Cfunc_blst_p2_deserialize /*line :2540:25*/)(&p2.cgo, (* /*line :2540:38*/_Ctype_byte /*line :2540:44*/)(&in[0])) != ( /*line :2540:58*/_Ciconst_BLST_SUCCESS /*line :2540:71*/) {
		return nil
	}
	return p2
}
func (p2 *P2Affine) Compress() []byte {
	var out [BLST_P2_COMPRESS_BYTES]byte
	( /*line :2547:2*/_Cfunc_blst_p2_affine_compress /*line :2547:26*/)((* /*line :2547:30*/_Ctype_byte /*line :2547:36*/)(&out[0]), &p2.cgo)
	return out[:]
}

func (p2 *P2Affine) Uncompress(in []byte) *P2Affine {
	if len(in) != BLST_P2_COMPRESS_BYTES {
		return nil
	}
	if ( /*line :2555:5*/_Cfunc_blst_p2_uncompress /*line :2555:24*/)(&p2.cgo, (* /*line :2555:37*/_Ctype_byte /*line :2555:43*/)(&in[0])) != ( /*line :2555:57*/_Ciconst_BLST_SUCCESS /*line :2555:70*/) {
		return nil
	}
	return p2
}

func (p2 *P2Affine) InG2() bool {
	return bool(( /*line :2562:14*/_Cfunc_blst_p2_affine_in_g2 /*line :2562:35*/)(&p2.cgo))
}

func (*P2Affine) BatchUncompress(in [][]byte) []*P2Affine {
	// Allocate space for all of the resulting points. Later we'll save pointers
	// and return those so that the result could be used in other functions,
	// such as MultipleAggregateVerify.
	n := len(in)
	points := make([]P2Affine, n)
	pointsPtrs := make([]*P2Affine, n)

	numThreads := numThreads(n)

	// Each thread will determine next message to process by atomically
	// incrementing curItem, process corresponding point, and
	// repeat until n is exceeded. Each thread will send a result (true for
	// success, false for failure) into the channel when complete.
	resCh := make(chan bool, numThreads)
	valid := int32(1)
	curItem := uint32(0)
	for tid := 0; tid < numThreads; tid++ {
		go func() {
			for atomic.LoadInt32(&valid) > 0 {
				// Get a work item
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(n) {
					break
				}
				if points[work].Uncompress(in[work]) == nil {
					atomic.StoreInt32(&valid, 0)
					break
				}
				pointsPtrs[work] = &points[work]
			}
			if atomic.LoadInt32(&valid) > 0 {
				resCh <- true
			} else {
				resCh <- false
			}
		}()
	}

	// Collect the threads
	result := true
	for i := 0; i < numThreads; i++ {
		if !<-resCh {
			result = false
		}
	}
	if atomic.LoadInt32(&valid) == 0 || !result {
		return nil
	}
	return pointsPtrs
}

func (p2 *P2) Serialize() []byte {
	var out [BLST_P2_SERIALIZE_BYTES]byte
	( /*line :2619:2*/_Cfunc_blst_p2_serialize /*line :2619:20*/)((* /*line :2619:24*/_Ctype_byte /*line :2619:30*/)(&out[0]), &p2.cgo)
	return out[:]
}
func (p2 *P2) Compress() []byte {
	var out [BLST_P2_COMPRESS_BYTES]byte
	( /*line :2624:2*/_Cfunc_blst_p2_compress /*line :2624:19*/)((* /*line :2624:23*/_Ctype_byte /*line :2624:29*/)(&out[0]), &p2.cgo)
	return out[:]
}

func (p2 *P2) MultAssign(scalarIf interface{}, optional ...int) *P2 {
	var nbits int
	var scalar * /*line :2630:14*/_Ctype_byte /*line :2630:20*/
	switch val := scalarIf.(type) {
	case []byte:
		scalar = (* /*line :2633:14*/_Ctype_byte /*line :2633:20*/)(&val[0])
		nbits = len(val) * 8
	case *Scalar:
		scalar = &val.cgo.b[0]
		nbits = 255
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}
	if len(optional) > 0 {
		nbits = optional[0]
	}
	( /*line :2644:2*/_Cfunc_blst_p2_mult /*line :2644:15*/)(&p2.cgo, &p2.cgo, scalar,  /*line :2644:43*/_Ctype_size_t /*line :2644:51*/(nbits))
	return p2
}

func (p2 *P2) Mult(scalarIf interface{}, optional ...int) *P2 {
	ret := *p2
	return ret.MultAssign(scalarIf, optional...)
}

func (p2 *P2) AddAssign(pointIf interface{}) *P2 {
	switch val := pointIf.(type) {
	case *P2:
		( /*line :2656:3*/_Cfunc_blst_p2_add_or_double /*line :2656:25*/)(&p2.cgo, &p2.cgo, &val.cgo)
	case *P2Affine:
		( /*line :2658:3*/_Cfunc_blst_p2_add_or_double_affine /*line :2658:32*/)(&p2.cgo, &p2.cgo, &val.cgo)
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}
	return p2
}

func (p2 *P2) Add(pointIf interface{}) *P2 {
	ret := *p2
	return ret.AddAssign(pointIf)
}

func (p2 *P2) SubAssign(pointIf interface{}) *P2 {
	var x * /*line :2671:9*/_Ctype_blst_fp2 /*line :2671:19*/
	var affine  /*line :2672:13*/_Ctype_bool /*line :2672:19*/
	switch val := pointIf.(type) {
	case *P2:
		x = &val.cgo.x
		affine = false
	case *P2Affine:
		x = &val.cgo.x
		affine = true
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}
	( /*line :2683:2*/_Cfunc_go_p2_sub_assign /*line :2683:19*/)(&p2.cgo, x, affine)
	return p2
}

func (p2 *P2) Sub(pointIf interface{}) *P2 {
	ret := *p2
	return ret.SubAssign(pointIf)
}

func P2Generator() *P2 {
	return &cgo_p2Generator
}

// 'acc += point * scalar', passing 'nil' for 'point' means "use the
//
//	group generator point"
func (acc *P2) MultNAccumulate(pointIf interface{}, scalarIf interface{},
	optional ...int) *P2 {
	var x * /*line :2701:9*/_Ctype_blst_fp2 /*line :2701:19*/
	var affine  /*line :2702:13*/_Ctype_bool /*line :2702:19*/
	if pointIf != nil {
		switch val := pointIf.(type) {
		case *P2:
			x = &val.cgo.x
			affine = false
		case *P2Affine:
			x = &val.cgo.x
			affine = true
		default:
			panic(fmt.Sprintf("unsupported type %T", val))
		}
	}
	var nbits int
	var scalar * /*line :2716:14*/_Ctype_byte /*line :2716:20*/
	switch val := scalarIf.(type) {
	case []byte:
		scalar = (* /*line :2719:14*/_Ctype_byte /*line :2719:20*/)(&val[0])
		nbits = len(val) * 8
	case *Scalar:
		scalar = &val.cgo.b[0]
		nbits = 255
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}
	if len(optional) > 0 {
		nbits = optional[0]
	}
	( /*line :2730:2*/_Cfunc_go_p2_mult_n_acc /*line :2730:19*/)(&acc.cgo, x, affine, scalar,  /*line :2730:50*/_Ctype_size_t /*line :2730:58*/(nbits))
	return acc
}

//
// Affine
//

func (p *P2) ToAffine() *P2Affine {
	var pa P2Affine
	( /*line :2740:2*/_Cfunc_blst_p2_to_affine /*line :2740:20*/)(&pa.cgo, &p.cgo)
	return &pa
}

func (p *P2) FromAffine(pa *P2Affine) {
	( /*line :2745:2*/_Cfunc_blst_p2_from_affine /*line :2745:22*/)(&p.cgo, &pa.cgo)
}

// Hash
func HashToG2(msg []byte, dst []byte,
	optional ...[]byte) *P2 { // aug
	var q P2

	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	( /*line :2758:2*/_Cfunc_blst_hash_to_g2 /*line :2758:18*/)(&q.cgo, ptrOrNil(msg),  /*line :2758:43*/_Ctype_size_t /*line :2758:51*/(len(msg)),
		ptrOrNil(dst),  /*line :2759:18*/_Ctype_size_t /*line :2759:26*/(len(dst)),
		ptrOrNil(aug),  /*line :2760:18*/_Ctype_size_t /*line :2760:26*/(len(aug)))
	return &q
}

func EncodeToG2(msg []byte, dst []byte,
	optional ...[]byte) *P2 { // aug
	var q P2

	var aug []byte
	if len(optional) > 0 {
		aug = optional[0]
	}

	( /*line :2773:2*/_Cfunc_blst_encode_to_g2 /*line :2773:20*/)(&q.cgo, ptrOrNil(msg),  /*line :2773:45*/_Ctype_size_t /*line :2773:53*/(len(msg)),
		ptrOrNil(dst),  /*line :2774:18*/_Ctype_size_t /*line :2774:26*/(len(dst)),
		ptrOrNil(aug),  /*line :2775:18*/_Ctype_size_t /*line :2775:26*/(len(aug)))
	return &q
}

//
// Multi-point/scalar operations
//

func P2sToAffine(points []*P2, optional ...int) P2Affines {
	var npoints int
	if len(optional) > 0 {
		npoints = optional[0]
	} else {
		npoints = len(points)
	}
	ret := make([]P2Affine, npoints)
	_cgoCheckPointer := func(...interface{}) {}
	func() { var _cgo0 *_Ctype_struct___5 = /*line :2792:23*/&ret[0].cgo; _cgoIndex1 := &/*line :2792:66*/points; _cgo1 := /*line :2792:36*/(**_Ctype_blst_p2 /*line :2792:48*/)(unsafe.Pointer(&(*_cgoIndex1)[0])); var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :2793:11*/(npoints); _cgoCheckPointer(_cgo1, *_cgoIndex1); /*line :2793:21*/_Cfunc_blst_p2s_to_affine(_cgo0, _cgo1, _cgo2); }()
	return ret
}

func (points P2s) ToAffine(optional ...P2Affines) P2Affines {
	npoints := len(points)
	var ret P2Affines

	if len(optional) > 0 { // used in benchmark
		ret = optional[0]
		if len(ret) < npoints {
			panic("npoints mismatch")
		}
	} else {
		ret = make([]P2Affine, npoints)
	}

	if maxProcs < 2 || npoints < 768 {
		( /*line :2811:3*/_Cfunc_go_p2slice_to_affine /*line :2811:24*/)(&ret[0].cgo, &points[0].cgo,  /*line :2811:55*/_Ctype_size_t /*line :2811:63*/(npoints))
		return ret
	}

	nslices := (npoints + 511) / 512
	if nslices > maxProcs {
		nslices = maxProcs
	}
	delta, rem := npoints/nslices+1, npoints%nslices

	var wg sync.WaitGroup
	wg.Add(nslices)
	for x := 0; x < npoints; x += delta {
		if rem == 0 {
			delta -= 1
		}
		rem -= 1
		go func(out *P2Affine, inp *P2, delta int) {
			( /*line :2829:4*/_Cfunc_go_p2slice_to_affine /*line :2829:25*/)(&out.cgo, &inp.cgo,  /*line :2829:47*/_Ctype_size_t /*line :2829:55*/(delta))
			wg.Done()
		}(&ret[x], &points[x], delta)
	}
	wg.Wait()

	return ret
}

//
// Batch addition
//

func P2AffinesAdd(points []*P2Affine, optional ...int) *P2 {
	var npoints int
	if len(optional) > 0 {
		npoints = optional[0]
	} else {
		npoints = len(points)
	}
	var ret P2
	_cgoCheckPointer := func(...interface{}) {}
	func() { var _cgo0 *_Ctype_struct___8 = /*line :2851:17*/&ret.cgo; _cgoIndex1 := &/*line :2851:64*/points; _cgo1 := /*line :2851:27*/(**_Ctype_blst_p2_affine /*line :2851:46*/)(unsafe.Pointer(&(*_cgoIndex1)[0])); var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :2852:11*/(npoints); _cgoCheckPointer(_cgo1, *_cgoIndex1); /*line :2852:21*/_Cfunc_blst_p2s_add(_cgo0, _cgo1, _cgo2); }()
	return &ret
}

func (points P2Affines) Add() *P2 {
	npoints := len(points)
	if maxProcs < 2 || npoints < 768 {
		var ret P2
		( /*line :2860:3*/_Cfunc_go_p2slice_add /*line :2860:18*/)(&ret.cgo, &points[0].cgo,  /*line :2860:46*/_Ctype_size_t /*line :2860:54*/(npoints))
		return &ret
	}

	nslices := (npoints + 511) / 512
	if nslices > maxProcs {
		nslices = maxProcs
	}
	delta, rem := npoints/nslices+1, npoints%nslices

	msgs := make(chan P2, nslices)
	for x := 0; x < npoints; x += delta {
		if rem == 0 {
			delta -= 1
		}
		rem -= 1
		go func(points *P2Affine, delta int) {
			var ret P2
			( /*line :2878:4*/_Cfunc_go_p2slice_add /*line :2878:19*/)(&ret.cgo, &points.cgo,  /*line :2878:44*/_Ctype_size_t /*line :2878:52*/(delta))
			msgs <- ret
		}(&points[x], delta)
	}

	ret := <-msgs
	for i := 1; i < nslices; i++ {
		msg := <-msgs
		( /*line :2886:3*/_Cfunc_blst_p2_add_or_double /*line :2886:25*/)(&ret.cgo, &ret.cgo, &msg.cgo)
	}
	return &ret
}

func (points P2s) Add() *P2 {
	return points.ToAffine().Add()
}

//
// Multi-scalar multiplication
//

func P2AffinesMult(pointsIf interface{}, scalarsIf interface{}, nbits int) *P2 {
	var npoints int
	switch val := pointsIf.(type) {
	case []*P2Affine:
		npoints = len(val)
	case []P2Affine:
		npoints = len(val)
	case P2Affines:
		npoints = len(val)
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}

	nbytes := (nbits + 7) / 8
	var scalars []* /*line :2913:17*/_Ctype_byte /*line :2913:23*/
	switch val := scalarsIf.(type) {
	case []byte:
		if len(val) < npoints*nbytes {
			return nil
		}
	case [][]byte:
		if len(val) < npoints {
			return nil
		}
		scalars = make([]* /*line :2923:21*/_Ctype_byte /*line :2923:27*/, npoints)
		for i := range scalars {
			scalars[i] = (* /*line :2925:19*/_Ctype_byte /*line :2925:25*/)(&val[i][0])
		}
	case []Scalar:
		if len(val) < npoints {
			return nil
		}
		if nbits <= 248 {
			scalars = make([]* /*line :2932:22*/_Ctype_byte /*line :2932:28*/, npoints)
			for i := range scalars {
				scalars[i] = &val[i].cgo.b[0]
			}
		}
	case []*Scalar:
		if len(val) < npoints {
			return nil
		}
		scalars = make([]* /*line :2941:21*/_Ctype_byte /*line :2941:27*/, npoints)
		for i := range scalars {
			scalars[i] = &val[i].cgo.b[0]
		}
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}

	numThreads := numThreads(0)

	if numThreads < 2 {
		sz := int(( /*line :2952:13*/_Cfunc_blst_p2s_mult_pippenger_scratch_sizeof /*line :2952:52*/)( /*line :2952:54*/_Ctype_size_t /*line :2952:62*/(npoints))) / 8
		scratch := make([]uint64, sz)

		pointsBySlice := [2]* /*line :2955:24*/_Ctype_blst_p2_affine /*line :2955:40*/{nil, nil}
		var p_points ** /*line :2956:18*/_Ctype_blst_p2_affine /*line :2956:34*/
		switch val := pointsIf.(type) {
		case []*P2Affine:
			p_points = (** /*line :2959:18*/_Ctype_blst_p2_affine /*line :2959:34*/)(unsafe.Pointer(&val[0]))
		case []P2Affine:
			pointsBySlice[0] = &val[0].cgo
			p_points = &pointsBySlice[0]
		case P2Affines:
			pointsBySlice[0] = &val[0].cgo
			p_points = &pointsBySlice[0]
		}

		scalarsBySlice := [2]* /*line :2968:25*/_Ctype_byte /*line :2968:31*/{nil, nil}
		var p_scalars ** /*line :2969:19*/_Ctype_byte /*line :2969:25*/
		switch val := scalarsIf.(type) {
		case []byte:
			scalarsBySlice[0] = (* /*line :2972:26*/_Ctype_byte /*line :2972:32*/)(&val[0])
			p_scalars = &scalarsBySlice[0]
		case [][]byte:
			p_scalars = &scalars[0]
		case []Scalar:
			if nbits > 248 {
				scalarsBySlice[0] = &val[0].cgo.b[0]
				p_scalars = &scalarsBySlice[0]
			} else {
				p_scalars = &scalars[0]
			}
		case []*Scalar:
			p_scalars = &scalars[0]
		}

		var ret P2
		_cgoCheckPointer := func(...interface{}) {}
		func() { var _cgo0 *_Ctype_struct___8 = /*line :2989:29*/&ret.cgo; _cgo1 := /*line :2989:39*/p_points; var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :2989:57*/(npoints); _cgo3 := /*line :2990:4*/p_scalars; var _cgo4 _Ctype_size_t = _Ctype_size_t /*line :2990:23*/(nbits); var _cgo5 *_Ctype_limb_t = /*line :2991:4*/(*_Ctype_limb_t /*line :2991:14*/)(&scratch[0]); _cgoCheckPointer(_cgo1, nil); _cgoCheckPointer(_cgo3, nil); /*line :2991:29*/_Cfunc_blst_p2s_mult_pippenger(_cgo0, _cgo1, _cgo2, _cgo3, _cgo4, _cgo5); }()

		for i := range scalars {
			scalars[i] = nil
		}

		return &ret
	}

	if npoints < 32 {
		if numThreads > npoints {
			numThreads = npoints
		}

		curItem := uint32(0)
		msgs := make(chan P2, numThreads)

		for tid := 0; tid < numThreads; tid++ {
			go func() {
				var acc P2

				for {
					workItem := int(atomic.AddUint32(&curItem, 1) - 1)
					if workItem >= npoints {
						break
					}

					var point *P2Affine
					switch val := pointsIf.(type) {
					case []*P2Affine:
						point = val[workItem]
					case []P2Affine:
						point = &val[workItem]
					case P2Affines:
						point = &val[workItem]
					}

					var scalar * /*line :3028:18*/_Ctype_byte /*line :3028:24*/
					switch val := scalarsIf.(type) {
					case []byte:
						scalar = (* /*line :3031:18*/_Ctype_byte /*line :3031:24*/)(&val[workItem*nbytes])
					case [][]byte:
						scalar = scalars[workItem]
					case []Scalar:
						if nbits > 248 {
							scalar = &val[workItem].cgo.b[0]
						} else {
							scalar = scalars[workItem]
						}
					case []*Scalar:
						scalar = scalars[workItem]
					}

					( /*line :3044:6*/_Cfunc_go_p2_mult_n_acc /*line :3044:23*/)(&acc.cgo, &point.cgo.x, true,
						scalar,  /*line :3045:15*/_Ctype_size_t /*line :3045:23*/(nbits))
				}

				msgs <- acc
			}()
		}

		ret := <-msgs
		for tid := 1; tid < numThreads; tid++ {
			point := <-msgs
			( /*line :3055:4*/_Cfunc_blst_p2_add_or_double /*line :3055:26*/)(&ret.cgo, &ret.cgo, &point.cgo)
		}

		for i := range scalars {
			scalars[i] = nil
		}

		return &ret
	}

	// this is sizeof(scratch[0])
	sz := int(( /*line :3066:12*/_Cfunc_blst_p2s_mult_pippenger_scratch_sizeof /*line :3066:51*/)(0)) / 8

	nx, ny, window := breakdown(nbits, pippenger_window_size(npoints),
		numThreads)

	// |grid[]| holds "coordinates" and place for result
	grid := make([]struct {
		x, dx, y, dy int
		point        P2
	}, nx*ny)

	dx := npoints / nx
	y := window * (ny - 1)
	total := 0
	for ; total < nx; total++ {
		grid[total].x = total * dx
		grid[total].dx = dx
		grid[total].y = y
		grid[total].dy = nbits - y
	}
	grid[total-1].dx = npoints - grid[total-1].x

	for y > 0 {
		y -= window
		for i := 0; i < nx; i++ {
			grid[total].x = grid[i].x
			grid[total].dx = grid[i].dx
			grid[total].y = y
			grid[total].dy = window
			total++
		}
	}

	if numThreads > total {
		numThreads = total
	}

	msgsCh := make(chan int, ny)
	rowSync := make([]int32, ny) // count up to |nx|
	curItem := int32(0)
	for tid := 0; tid < numThreads; tid++ {
		go func() {
			scratch := make([]uint64, sz<<uint(window-1))
			pointsBySlice := [2]* /*line :3109:25*/_Ctype_blst_p2_affine /*line :3109:41*/{nil, nil}
			scalarsBySlice := [2]* /*line :3110:26*/_Ctype_byte /*line :3110:32*/{nil, nil}
			_cgoCheckPointer := func(...interface{}) {}

			for {
				workItem := atomic.AddInt32(&curItem, 1) - 1
				if int(workItem) >= total {
					break
				}

				x := grid[workItem].x
				y := grid[workItem].y

				var p_points ** /*line :3122:20*/_Ctype_blst_p2_affine /*line :3122:36*/
				switch val := pointsIf.(type) {
				case []*P2Affine:
					p_points = (** /*line :3125:20*/_Ctype_blst_p2_affine /*line :3125:36*/)(unsafe.Pointer(&val[x]))
				case []P2Affine:
					pointsBySlice[0] = &val[x].cgo
					p_points = &pointsBySlice[0]
				case P2Affines:
					pointsBySlice[0] = &val[x].cgo
					p_points = &pointsBySlice[0]
				}

				var p_scalars ** /*line :3134:21*/_Ctype_byte /*line :3134:27*/
				switch val := scalarsIf.(type) {
				case []byte:
					scalarsBySlice[0] = (* /*line :3137:28*/_Ctype_byte /*line :3137:34*/)(&val[x*nbytes])
					p_scalars = &scalarsBySlice[0]
				case [][]byte:
					p_scalars = &scalars[x]
				case []Scalar:
					if nbits > 248 {
						scalarsBySlice[0] = &val[x].cgo.b[0]
						p_scalars = &scalarsBySlice[0]
					} else {
						p_scalars = &scalars[x]
					}
				case []*Scalar:
					p_scalars = &scalars[x]
				}

				func() { var _cgo0 *_Ctype_struct___8 = /*line :3152:31*/&grid[workItem].point.cgo; _cgo1 := /*line :3153:6*/p_points; var _cgo2 _Ctype_size_t = _Ctype_size_t /*line :3153:24*/(grid[workItem].dx); _cgo3 := /*line :3154:6*/p_scalars; var _cgo4 _Ctype_size_t = _Ctype_size_t /*line :3154:25*/(nbits); var _cgo5 *_Ctype_limb_t = /*line :3155:6*/(*_Ctype_limb_t /*line :3155:16*/)(&scratch[0]); var _cgo6 _Ctype_size_t = _Ctype_size_t /*line :3156:14*/(y); var _cgo7 _Ctype_size_t = _Ctype_size_t /*line :3156:27*/(window); _cgoCheckPointer(_cgo1, nil); _cgoCheckPointer(_cgo3, nil); /*line :3156:36*/_Cfunc_blst_p2s_tile_pippenger(_cgo0, _cgo1, _cgo2, _cgo3, _cgo4, _cgo5, _cgo6, _cgo7); }()

				if atomic.AddInt32(&rowSync[y/window], 1) == int32(nx) {
					msgsCh <- y // "row" is done
				} else {
					runtime.Gosched() // be nice to the application
				}
			}

			pointsBySlice[0] = nil
			scalarsBySlice[0] = nil
		}()
	}

	var ret P2
	rows := make([]bool, ny)
	row := 0                  // actually index in |grid[]|
	for i := 0; i < ny; i++ { // we expect |ny| messages, one per "row"
		y := <-msgsCh
		rows[y/window] = true  // mark the "row"
		for grid[row].y == y { // if it's current "row", process it
			for row < total && grid[row].y == y {
				( /*line :3178:5*/_Cfunc_blst_p2_add_or_double /*line :3178:27*/)(&ret.cgo, &ret.cgo, &grid[row].point.cgo)
				row++
			}
			if y == 0 {
				break // one can as well 'return &ret' here
			}
			for j := 0; j < window; j++ {
				( /*line :3185:5*/_Cfunc_blst_p2_double /*line :3185:20*/)(&ret.cgo, &ret.cgo)
			}
			y -= window
			if !rows[y/window] { // see if next "row" was marked already
				break
			}
		}
	}

	for i := range scalars {
		scalars[i] = nil
	}

	return &ret
}

func (points P2Affines) Mult(scalarsIf interface{}, nbits int) *P2 {
	return P2AffinesMult(points, scalarsIf, nbits)
}

func (points P2s) Mult(scalarsIf interface{}, nbits int) *P2 {
	return points.ToAffine().Mult(scalarsIf, nbits)
}

//
// Group-check
//

func P2AffinesValidate(pointsIf interface{}) bool {
	var npoints int
	switch val := pointsIf.(type) {
	case []*P2Affine:
		npoints = len(val)
	case []P2Affine:
		npoints = len(val)
	case P2Affines:
		npoints = len(val)
	default:
		panic(fmt.Sprintf("unsupported type %T", val))
	}

	numThreads := numThreads(npoints)

	if numThreads < 2 {
		for i := 0; i < npoints; i++ {
			var point *P2Affine

			switch val := pointsIf.(type) {
			case []*P2Affine:
				point = val[i]
			case []P2Affine:
				point = &val[i]
			case P2Affines:
				point = &val[i]
			default:
				panic(fmt.Sprintf("unsupported type %T", val))
			}

			if !( /*line :3243:8*/_Cfunc_go_p2_affine_validate /*line :3243:30*/)(&point.cgo, true) {
				return false
			}
		}

		return true
	}

	valid := int32(1)
	curItem := uint32(0)

	var wg sync.WaitGroup
	wg.Add(numThreads)

	for tid := 0; tid < numThreads; tid++ {
		go func() {
			for atomic.LoadInt32(&valid) != 0 {
				work := atomic.AddUint32(&curItem, 1) - 1
				if work >= uint32(npoints) {
					break
				}

				var point *P2Affine

				switch val := pointsIf.(type) {
				case []*P2Affine:
					point = val[work]
				case []P2Affine:
					point = &val[work]
				case P2Affines:
					point = &val[work]
				default:
					panic(fmt.Sprintf("unsupported type %T", val))
				}

				if !( /*line :3278:9*/_Cfunc_go_p2_affine_validate /*line :3278:31*/)(&point.cgo, true) {
					atomic.StoreInt32(&valid, 0)
					break
				}
			}

			wg.Done()
		}()
	}

	wg.Wait()

	return atomic.LoadInt32(&valid) != 0
}

func (points P2Affines) Validate() bool {
	return P2AffinesValidate(points)
}

// aug [][]byte - augmentation bytes for signing (default: nil)
func parseOpts(optional ...interface{}) (augSingle []byte, aug [][]byte,
	useHash bool, ok bool) {
	useHash = true // hash (true), encode (false)

	for _, arg := range optional {
		switch v := arg.(type) {
		case []byte:
			augSingle = v
		case [][]byte:
			aug = v
		case bool:
			useHash = v
		default:
			return nil, nil, useHash, false
		}
	}
	return augSingle, aug, useHash, true
}

// These methods are inefficient because of cgo call overhead. For this
// reason they should be used primarily for prototyping with a goal to
// formulate interfaces that would process multiple scalars per cgo call.
func (a *Scalar) MulAssign(b *Scalar) (*Scalar, bool) {
	return a, bool(( /*line :3321:17*/_Cfunc_blst_sk_mul_n_check /*line :3321:37*/)(&a.cgo, &a.cgo, &b.cgo))
}

func (a *Scalar) Mul(b *Scalar) (*Scalar, bool) {
	var ret Scalar
	return &ret, bool(( /*line :3326:20*/_Cfunc_blst_sk_mul_n_check /*line :3326:40*/)(&ret.cgo, &a.cgo, &b.cgo))
}

func (a *Scalar) AddAssign(b *Scalar) (*Scalar, bool) {
	return a, bool(( /*line :3330:17*/_Cfunc_blst_sk_add_n_check /*line :3330:37*/)(&a.cgo, &a.cgo, &b.cgo))
}

func (a *Scalar) Add(b *Scalar) (*Scalar, bool) {
	var ret Scalar
	return &ret, bool(( /*line :3335:20*/_Cfunc_blst_sk_add_n_check /*line :3335:40*/)(&ret.cgo, &a.cgo, &b.cgo))
}

func (a *Scalar) SubAssign(b *Scalar) (*Scalar, bool) {
	return a, bool(( /*line :3339:17*/_Cfunc_blst_sk_sub_n_check /*line :3339:37*/)(&a.cgo, &a.cgo, &b.cgo))
}

func (a *Scalar) Sub(b *Scalar) (*Scalar, bool) {
	var ret Scalar
	return &ret, bool(( /*line :3344:20*/_Cfunc_blst_sk_sub_n_check /*line :3344:40*/)(&ret.cgo, &a.cgo, &b.cgo))
}

func (a *Scalar) Inverse() *Scalar {
	var ret Scalar
	( /*line :3349:2*/_Cfunc_blst_sk_inverse /*line :3349:18*/)(&ret.cgo, &a.cgo)
	return &ret
}

//
// Serialization/Deserialization.
//

// Scalar serdes
func (s *Scalar) Serialize() []byte {
	var out [BLST_SCALAR_BYTES]byte
	( /*line :3360:2*/_Cfunc_blst_bendian_from_scalar /*line :3360:27*/)((* /*line :3360:31*/_Ctype_byte /*line :3360:37*/)(&out[0]), &s.cgo)
	return out[:]
}

func (s *Scalar) Deserialize(in []byte) *Scalar {
	if len(in) != BLST_SCALAR_BYTES ||
		!( /*line :3366:4*/_Cfunc_go_scalar_from_bendian /*line :3366:27*/)(&s.cgo, (* /*line :3366:39*/_Ctype_byte /*line :3366:45*/)(&in[0])) {
		return nil
	}
	return s
}

func (s *Scalar) Valid() bool {
	return bool(( /*line :3373:14*/_Cfunc_blst_sk_check /*line :3373:28*/)(&s.cgo))
}

func (s *Scalar) HashTo(msg []byte, dst []byte) bool {
	ret := HashToScalar(msg, dst)
	if ret != nil {
		*s = *ret
		return true
	}
	return false
}

func HashToScalar(msg []byte, dst []byte) *Scalar {
	var ret Scalar

	if ( /*line :3388:5*/_Cfunc_go_hash_to_scalar /*line :3388:23*/)(&ret.cgo, ptrOrNil(msg),  /*line :3388:50*/_Ctype_size_t /*line :3388:58*/(len(msg)),
		ptrOrNil(dst),  /*line :3389:18*/_Ctype_size_t /*line :3389:26*/(len(dst))) {
		return &ret
	}

	return nil
}

//
// LEndian
//

func (fr *Scalar) ToLEndian() []byte {
	var arr [BLST_SCALAR_BYTES]byte
	( /*line :3402:2*/_Cfunc_blst_lendian_from_scalar /*line :3402:27*/)((* /*line :3402:31*/_Ctype_byte /*line :3402:37*/)(&arr[0]), &fr.cgo)
	return arr[:]
}

func (fp *Fp) ToLEndian() []byte {
	var arr [BLST_FP_BYTES]byte
	( /*line :3408:2*/_Cfunc_blst_lendian_from_fp /*line :3408:23*/)((* /*line :3408:27*/_Ctype_byte /*line :3408:33*/)(&arr[0]), &fp.cgo)
	return arr[:]
}

func (fr *Scalar) FromLEndian(arr []byte) *Scalar {
	nbytes := len(arr)
	if nbytes < BLST_SCALAR_BYTES ||
		!( /*line :3415:4*/_Cfunc_blst_scalar_from_le_bytes /*line :3415:30*/)(&fr.cgo, (* /*line :3415:43*/_Ctype_byte /*line :3415:49*/)(&arr[0]),  /*line :3415:61*/_Ctype_size_t /*line :3415:69*/(nbytes)) {
		return nil
	}
	return fr
}

func (fp *Fp) FromLEndian(arr []byte) *Fp {
	if len(arr) != BLST_FP_BYTES {
		return nil
	}
	( /*line :3425:2*/_Cfunc_blst_fp_from_lendian /*line :3425:23*/)(&fp.cgo, (* /*line :3425:36*/_Ctype_byte /*line :3425:42*/)(&arr[0]))
	return fp
}

//
// BEndian
//

func (fr *Scalar) ToBEndian() []byte {
	var arr [BLST_SCALAR_BYTES]byte
	( /*line :3435:2*/_Cfunc_blst_bendian_from_scalar /*line :3435:27*/)((* /*line :3435:31*/_Ctype_byte /*line :3435:37*/)(&arr[0]), &fr.cgo)
	return arr[:]
}

func (fp *Fp) ToBEndian() []byte {
	var arr [BLST_FP_BYTES]byte
	( /*line :3441:2*/_Cfunc_blst_bendian_from_fp /*line :3441:23*/)((* /*line :3441:27*/_Ctype_byte /*line :3441:33*/)(&arr[0]), &fp.cgo)
	return arr[:]
}

func (fr *Scalar) FromBEndian(arr []byte) *Scalar {
	nbytes := len(arr)
	if nbytes < BLST_SCALAR_BYTES ||
		!( /*line :3448:4*/_Cfunc_blst_scalar_from_be_bytes /*line :3448:30*/)(&fr.cgo, (* /*line :3448:43*/_Ctype_byte /*line :3448:49*/)(&arr[0]),  /*line :3448:61*/_Ctype_size_t /*line :3448:69*/(nbytes)) {
		return nil
	}
	return fr
}

func (fp *Fp) FromBEndian(arr []byte) *Fp {
	if len(arr) != BLST_FP_BYTES {
		return nil
	}
	( /*line :3458:2*/_Cfunc_blst_fp_from_bendian /*line :3458:23*/)(&fp.cgo, (* /*line :3458:36*/_Ctype_byte /*line :3458:42*/)(&arr[0]))
	return fp
}

//
// Printing
//

func PrintBytes(val []byte, name string) {
	fmt.Printf("%s = %02x\n", name, val)
}

func (s *Scalar) Print(name string) {
	arr := s.ToBEndian()
	PrintBytes(arr, name)
}

func (p *P1Affine) Print(name string) {
	fmt.Printf("%s:\n", name)
	x := Fp{p.cgo.x}
	arr := x.ToBEndian()
	PrintBytes(arr, "  x")
	y := Fp{p.cgo.y}
	arr = y.ToBEndian()
	PrintBytes(arr, "  y")
}

func (p *P1) Print(name string) {
	fmt.Printf("%s:\n", name)
	aff := p.ToAffine()
	aff.Print(name)
}

func (f *Fp2) Print(name string) {
	fmt.Printf("%s:\n", name)
	var arr [BLST_FP_BYTES]byte
	( /*line :3494:2*/_Cfunc_blst_bendian_from_fp /*line :3494:23*/)((* /*line :3494:27*/_Ctype_byte /*line :3494:33*/)(&arr[0]), &f.cgo.fp[0])
	PrintBytes(arr[:], "    0")
	( /*line :3496:2*/_Cfunc_blst_bendian_from_fp /*line :3496:23*/)((* /*line :3496:27*/_Ctype_byte /*line :3496:33*/)(&arr[0]), &f.cgo.fp[1])
	PrintBytes(arr[:], "    1")
}

func (p *P2Affine) Print(name string) {
	fmt.Printf("%s:\n", name)
	x := Fp2{p.cgo.x}
	x.Print("  x")
	y := Fp2{p.cgo.y}
	y.Print("  y")
}

func (p *P2) Print(name string) {
	fmt.Printf("%s:\n", name)
	aff := p.ToAffine()
	aff.Print(name)
}

//
// Equality
//

func (s1 *Scalar) Equals(s2 *Scalar) bool {
	return *s1 == *s2
}

func (e1 *Fp) Equals(e2 *Fp) bool {
	return *e1 == *e2
}

func (e1 *Fp2) Equals(e2 *Fp2) bool {
	return *e1 == *e2
}

func (e1 *P1Affine) Equals(e2 *P1Affine) bool {
	return bool(( /*line :3531:14*/_Cfunc_blst_p1_affine_is_equal /*line :3531:38*/)(&e1.cgo, &e2.cgo))
}

func (pt *P1Affine) asPtr() * /*line :3534:30*/_Ctype_blst_p1_affine /*line :3534:46*/ {
	if pt != nil {
		return &pt.cgo
	}

	return nil
}

func (e1 *P1) Equals(e2 *P1) bool {
	return bool(( /*line :3543:14*/_Cfunc_blst_p1_is_equal /*line :3543:31*/)(&e1.cgo, &e2.cgo))
}

func (e1 *P2Affine) Equals(e2 *P2Affine) bool {
	return bool(( /*line :3547:14*/_Cfunc_blst_p2_affine_is_equal /*line :3547:38*/)(&e1.cgo, &e2.cgo))
}

func (pt *P2Affine) asPtr() * /*line :3550:30*/_Ctype_blst_p2_affine /*line :3550:46*/ {
	if pt != nil {
		return &pt.cgo
	}

	return nil
}

func (e1 *P2) Equals(e2 *P2) bool {
	return bool(( /*line :3559:14*/_Cfunc_blst_p2_is_equal /*line :3559:31*/)(&e1.cgo, &e2.cgo))
}

// private thunk for testing

func expandMessageXmd(msg []byte, dst []byte, len_in_bytes int) []byte {
	ret := make([]byte, len_in_bytes)

	( /*line :3567:2*/_Cfunc_blst_expand_message_xmd /*line :3567:26*/)((* /*line :3567:30*/_Ctype_byte /*line :3567:36*/)(&ret[0]),  /*line :3567:48*/_Ctype_size_t /*line :3567:56*/(len(ret)),
		ptrOrNil(msg),  /*line :3568:18*/_Ctype_size_t /*line :3568:26*/(len(msg)),
		ptrOrNil(dst),  /*line :3569:18*/_Ctype_size_t /*line :3569:26*/(len(dst)))
	return ret
}

func breakdown(nbits, window, ncpus int) (nx int, ny int, wnd int) {

	if nbits > window*ncpus { //nolint:nestif
		nx = 1
		wnd = bits.Len(uint(ncpus) / 4)
		if (window + wnd) > 18 {
			wnd = window - wnd
		} else {
			wnd = (nbits/window + ncpus - 1) / ncpus
			if (nbits/(window+1)+ncpus-1)/ncpus < wnd {
				wnd = window + 1
			} else {
				wnd = window
			}
		}
	} else {
		nx = 2
		wnd = window - 2
		for (nbits/wnd+1)*nx < ncpus {
			nx += 1
			wnd = window - bits.Len(3*uint(nx)/2)
		}
		nx -= 1
		wnd = window - bits.Len(3*uint(nx)/2)
	}
	ny = nbits/wnd + 1
	wnd = nbits/ny + 1

	return nx, ny, wnd
}

func pippenger_window_size(npoints int) int {
	wbits := bits.Len(uint(npoints))

	if wbits > 13 {
		return wbits - 4
	}
	if wbits > 5 {
		return wbits - 3
	}
	return 2
}
